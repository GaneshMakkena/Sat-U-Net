{
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.7.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [
        {
          "sourceId": 1635643,
          "sourceType": "datasetVersion",
          "datasetId": 966962
        },
        {
          "sourceId": 4861537,
          "sourceType": "datasetVersion",
          "datasetId": 2818408
        }
      ],
      "dockerImageVersionId": 30357,
      "isInternetEnabled": false,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU"
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#  **Satellite Image Analysis Using Convolution Neural Networks For Environmental Monitoring**"
      ],
      "metadata": {
        "id": "u2QwcNCnwcd-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!pip install transformers\n",
        "!pip install albumentations opencv-python-headless"
      ],
      "metadata": {
        "id": "gqJ0rD7vQukz",
        "execution": {
          "iopub.status.busy": "2024-11-13T13:11:59.363100Z",
          "iopub.execute_input": "2024-11-13T13:11:59.363627Z",
          "iopub.status.idle": "2024-11-13T13:12:34.751010Z",
          "shell.execute_reply.started": "2024-11-13T13:11:59.363521Z",
          "shell.execute_reply": "2024-11-13T13:12:34.749139Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "fIEPUjYx2YZN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Data Manipulation\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "import albumentations as A\n",
        "import cv2\n",
        "\n",
        "# Visualization\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Computer Vision\n",
        "import cv2 as cv\n",
        "\n",
        "# CNN\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras import regularizers\n",
        "from tensorflow.keras.layers import Conv2D, Dropout, Dense, AveragePooling2D, MaxPooling2D, Flatten, Activation\n",
        "from tqdm.keras import TqdmCallback\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "# Vision transformers\n",
        "from transformers.models.vit.feature_extraction_vit import ViTFeatureExtractor\n",
        "from transformers import ViTForImageClassification, TrainingArguments, Trainer\n",
        "\n",
        "# Saving models and data\n",
        "import joblib\n",
        "\n",
        "# OS for navigation and environment set up\n",
        "import os\n",
        "\n",
        "# Tensorflow\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "\n",
        "# Splitting datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Random\n",
        "import random\n",
        "\n",
        "# Progress bar\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Model import\n",
        "\n",
        "from keras.models import load_model\n",
        "\n",
        "# Warnings\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore', category=DeprecationWarning)"
      ],
      "metadata": {
        "id": "thORb_-4xf3Z",
        "execution": {
          "iopub.status.busy": "2024-11-13T13:13:08.808349Z",
          "iopub.execute_input": "2024-11-13T13:13:08.808851Z",
          "iopub.status.idle": "2024-11-13T13:13:20.398248Z",
          "shell.execute_reply.started": "2024-11-13T13:13:08.808808Z",
          "shell.execute_reply": "2024-11-13T13:13:20.397116Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data import"
      ],
      "metadata": {
        "id": "k7-uwN5SwiW4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "base_dir = \"/content/drive/MyDrive/Satellite Dataset\"\n",
        "train_dir = os.path.join(base_dir, \"train\")\n",
        "test_dir = os.path.join(base_dir, \"test\")\n",
        "valid_dir = os.path.join(base_dir, \"valid\")\n",
        "annotations = pd.read_csv(os.path.join(base_dir, \"metadata.csv\"))\n",
        "classes = pd.read_csv(os.path.join(base_dir, \"class_dict.csv\"))"
      ],
      "metadata": {
        "id": "41edlGDiwgtP",
        "execution": {
          "iopub.status.busy": "2024-11-13T13:13:20.401729Z",
          "iopub.execute_input": "2024-11-13T13:13:20.402881Z",
          "iopub.status.idle": "2024-11-13T13:13:20.428853Z",
          "shell.execute_reply.started": "2024-11-13T13:13:20.402810Z",
          "shell.execute_reply": "2024-11-13T13:13:20.427902Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class_path = '/content/drive/MyDrive/Satellite Dataset/class_dict.csv'\n",
        "data_path = '/content/drive/MyDrive/Satellite Dataset/metadata.csv'\n",
        "\n",
        "df = annotations\n",
        "\n",
        "train_data_paths = df[df['split']=='train']\n",
        "test_data_paths = df[df['split']=='test']\n",
        "\n",
        "train_data_paths.head()"
      ],
      "metadata": {
        "id": "vuIZGXeowlUe",
        "execution": {
          "iopub.status.busy": "2024-11-13T13:13:20.430209Z",
          "iopub.execute_input": "2024-11-13T13:13:20.430542Z",
          "iopub.status.idle": "2024-11-13T13:13:20.463508Z",
          "shell.execute_reply.started": "2024-11-13T13:13:20.430512Z",
          "shell.execute_reply": "2024-11-13T13:13:20.462416Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now we have to concatenate all the train relative paths with the base dir."
      ],
      "metadata": {
        "id": "r58HgW3fwqRK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Run and Ignore the warnings\n",
        "train_data_paths['sat_image_path'] = pd.Series([os.path.join(base_dir, path) for path in train_data_paths['sat_image_path']])\n",
        "train_data_paths['mask_path'] = pd.Series([os.path.join(base_dir, path) for path in train_data_paths['mask_path']])"
      ],
      "metadata": {
        "id": "jOHG9d4Wwt1d",
        "execution": {
          "iopub.status.busy": "2024-11-13T13:16:10.692780Z",
          "iopub.execute_input": "2024-11-13T13:16:10.693381Z",
          "iopub.status.idle": "2024-11-13T13:16:10.713675Z",
          "shell.execute_reply.started": "2024-11-13T13:16:10.693339Z",
          "shell.execute_reply": "2024-11-13T13:16:10.712392Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_paths"
      ],
      "metadata": {
        "id": "TQHsUlDwtzSi",
        "execution": {
          "iopub.status.busy": "2024-11-13T13:16:17.530760Z",
          "iopub.execute_input": "2024-11-13T13:16:17.531625Z",
          "iopub.status.idle": "2024-11-13T13:16:17.548786Z",
          "shell.execute_reply.started": "2024-11-13T13:16:17.531584Z",
          "shell.execute_reply": "2024-11-13T13:16:17.547464Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "labels = pd.read_csv(class_path)\n",
        "labels"
      ],
      "metadata": {
        "id": "NjRKIb4uW_9T",
        "execution": {
          "iopub.status.busy": "2024-11-13T13:16:23.500960Z",
          "iopub.execute_input": "2024-11-13T13:16:23.502144Z",
          "iopub.status.idle": "2024-11-13T13:16:23.520789Z",
          "shell.execute_reply.started": "2024-11-13T13:16:23.502089Z",
          "shell.execute_reply": "2024-11-13T13:16:23.519390Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Class ID\tClass Name\t      RGB Value\t          Color Name\n",
        "0\t        Urban Land\t      (0, 255, 255)\t      Cyan\n",
        "1\t        Agriculture Land\t(255, 255, 0)\t      Yellow\n",
        "2\t        Rangeland\t        (255, 0, 255)\t      Magenta\n",
        "3\t        Forest Land\t      (0, 255, 0)\t        Green\n",
        "4\t        Water\t            (0, 0, 255)\t        Blue\n",
        "5\t        Barren Land\t      (255, 255, 255)\t    White\n",
        "6\t        Unknown\t          (0, 0, 0)\t          Black"
      ],
      "metadata": {
        "id": "bRIc9ic-fXQW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Notice that the land segmentation is given by:\n",
        "\n",
        "| Class  | Colour |\n",
        "| --- | --- |\n",
        "| `urban_land` |  cyan |\n",
        "| `agriculture_land` | yellow |\n",
        "| `rangeland` | magenta |\n",
        "| `forest_land` | Green |\n",
        "| `water` | Blue  |\n",
        "| `barren_land` | White |\n",
        "| `unknown` | Black |\n"
      ],
      "metadata": {
        "id": "8vbEyFm21ZM2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data-viz"
      ],
      "metadata": {
        "id": "xzFowXlJwllq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_path = train_data_paths['sat_image_path'][3]\n",
        "sample_image = cv.imread(sample_path)\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "plt.title('Satellital image')\n",
        "plt.imshow(sample_image)\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "yPAYov83YF75",
        "execution": {
          "iopub.status.busy": "2024-11-13T13:16:30.842465Z",
          "iopub.execute_input": "2024-11-13T13:16:30.842955Z",
          "iopub.status.idle": "2024-11-13T13:16:32.147614Z",
          "shell.execute_reply.started": "2024-11-13T13:16:30.842916Z",
          "shell.execute_reply": "2024-11-13T13:16:32.146193Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample1 = train_data_paths['sat_image_path'][:200]\n",
        "sample2 = train_data_paths['mask_path'][:200]\n",
        "\n",
        "i=-2\n",
        "\n",
        "fig, ax = plt.subplots(ncols=20, nrows=10, figsize=(20, 10))\n",
        "for x in range(10):\n",
        "  i+=1\n",
        "  for y in range(20):\n",
        "    i+=1\n",
        "    if i%2==0:\n",
        "      path = sample1[5*x+y]\n",
        "    else:\n",
        "      path = sample2[5*x+y]\n",
        "    image = cv.imread(path)\n",
        "    ax[x, y].imshow(image)\n",
        "    ax[x, y].axis('off')\n",
        "plt.suptitle('Satellital images')\n",
        "fig.show()"
      ],
      "metadata": {
        "id": "GXf1AfuLacqO",
        "execution": {
          "iopub.status.busy": "2024-11-13T13:16:38.182363Z",
          "iopub.execute_input": "2024-11-13T13:16:38.183157Z",
          "iopub.status.idle": "2024-11-13T13:19:27.046189Z",
          "shell.execute_reply.started": "2024-11-13T13:16:38.183115Z",
          "shell.execute_reply": "2024-11-13T13:19:27.044930Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_overlay(index_img=0, data_path=train_data_paths):\n",
        "  # Load the image and the mask\n",
        "  img = plt.imread(data_path['sat_image_path'][index_img])\n",
        "  mask = plt.imread(data_path['mask_path'][index_img])\n",
        "\n",
        "  # Create a figure and axes\n",
        "  fig, axs = plt.subplots(1,3, figsize=(24,8))\n",
        "\n",
        "  # Display the sat image\n",
        "  axs[0].imshow(img)\n",
        "  axs[0].set_title(\"Sat Image\")\n",
        "  axs[0].axis('off')\n",
        "\n",
        "  # Display the mask\n",
        "  axs[1].imshow(mask)\n",
        "  axs[1].set_title(\"Mask\")\n",
        "  axs[1].axis('off')\n",
        "\n",
        "  # Display the mask over the sat image\n",
        "  axs[2].imshow(img)\n",
        "  axs[2].imshow(mask, alpha=0.3)\n",
        "  axs[2].set_title(\"Overlay\")\n",
        "  axs[2].axis('off')\n",
        "\n",
        "  # show the plot\n",
        "  plt.show()"
      ],
      "metadata": {
        "id": "VmZV1G8az96G",
        "execution": {
          "iopub.status.busy": "2024-11-13T13:19:27.048813Z",
          "iopub.execute_input": "2024-11-13T13:19:27.049316Z",
          "iopub.status.idle": "2024-11-13T13:19:27.063269Z",
          "shell.execute_reply.started": "2024-11-13T13:19:27.049271Z",
          "shell.execute_reply": "2024-11-13T13:19:27.062019Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Observe how segmentations are displayed:"
      ],
      "metadata": {
        "id": "8x4vuv_2ujk2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "for i in random.sample(range(len(train_data_paths)), 5):\n",
        "  plot_overlay(index_img=i)"
      ],
      "metadata": {
        "id": "CvQw26wk1soX",
        "execution": {
          "iopub.status.busy": "2024-11-13T13:19:27.064656Z",
          "iopub.execute_input": "2024-11-13T13:19:27.065047Z",
          "iopub.status.idle": "2024-11-13T13:20:16.969490Z",
          "shell.execute_reply.started": "2024-11-13T13:19:27.065011Z",
          "shell.execute_reply": "2024-11-13T13:20:16.968092Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Data preprocessing"
      ],
      "metadata": {
        "id": "be7p1uhAwnY-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## One Hot **Encoding**"
      ],
      "metadata": {
        "id": "o9SUX-nVwLaB"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Target: convert mask rgb values into one hot encoding vectors."
      ],
      "metadata": {
        "id": "L3w7MFW9JoyC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_data_paths['mask_path'][8]"
      ],
      "metadata": {
        "id": "fdvoFVC1t6yX",
        "execution": {
          "iopub.status.busy": "2024-11-13T13:20:16.973314Z",
          "iopub.execute_input": "2024-11-13T13:20:16.974458Z",
          "iopub.status.idle": "2024-11-13T13:20:16.982756Z",
          "shell.execute_reply.started": "2024-11-13T13:20:16.974409Z",
          "shell.execute_reply": "2024-11-13T13:20:16.981590Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_image = plt.imread(train_data_paths['mask_path'][8])\n",
        "\n",
        "plt.figure(figsize=(10, 10))\n",
        "plt.title('Example of Mask Image')\n",
        "plt.axis('off')\n",
        "plt.imshow(sample_image)\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "BFfJzWoCJSz2",
        "execution": {
          "iopub.status.busy": "2024-11-13T13:20:16.984292Z",
          "iopub.execute_input": "2024-11-13T13:20:16.984679Z",
          "iopub.status.idle": "2024-11-13T13:20:18.668232Z",
          "shell.execute_reply.started": "2024-11-13T13:20:16.984645Z",
          "shell.execute_reply": "2024-11-13T13:20:18.666830Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_image[45][68]"
      ],
      "metadata": {
        "id": "BFcYL5WgMu1l",
        "execution": {
          "iopub.status.busy": "2024-11-13T13:20:18.669630Z",
          "iopub.execute_input": "2024-11-13T13:20:18.670018Z",
          "iopub.status.idle": "2024-11-13T13:20:18.678437Z",
          "shell.execute_reply.started": "2024-11-13T13:20:18.669982Z",
          "shell.execute_reply": "2024-11-13T13:20:18.677266Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "uniques = np.array([sample_image[i][j] for i in range(2448) for j in range(2448)])\n",
        "uniques = np.unique(uniques)"
      ],
      "metadata": {
        "id": "C-gVA305LtJA",
        "execution": {
          "iopub.status.busy": "2024-11-13T13:20:18.679834Z",
          "iopub.execute_input": "2024-11-13T13:20:18.680243Z",
          "iopub.status.idle": "2024-11-13T13:20:27.362125Z",
          "shell.execute_reply.started": "2024-11-13T13:20:18.680209Z",
          "shell.execute_reply": "2024-11-13T13:20:27.360790Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "uniques"
      ],
      "metadata": {
        "id": "ig-9XYswKpfW",
        "execution": {
          "iopub.status.busy": "2024-11-13T13:20:27.363717Z",
          "iopub.execute_input": "2024-11-13T13:20:27.364132Z",
          "iopub.status.idle": "2024-11-13T13:20:27.372508Z",
          "shell.execute_reply.started": "2024-11-13T13:20:27.364095Z",
          "shell.execute_reply": "2024-11-13T13:20:27.371192Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Image Tiling"
      ],
      "metadata": {
        "id": "xUu10T1QwNVL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sample_path = train_data_paths['sat_image_path'][0]\n",
        "sample_image = plt.imread(sample_path)"
      ],
      "metadata": {
        "id": "x3rHDHqfwQhX",
        "execution": {
          "iopub.status.busy": "2024-11-13T13:20:27.374300Z",
          "iopub.execute_input": "2024-11-13T13:20:27.375182Z",
          "iopub.status.idle": "2024-11-13T13:20:27.522073Z",
          "shell.execute_reply.started": "2024-11-13T13:20:27.375134Z",
          "shell.execute_reply": "2024-11-13T13:20:27.521036Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_image.shape"
      ],
      "metadata": {
        "id": "oCz2Tzqlwrpz",
        "execution": {
          "iopub.status.busy": "2024-11-13T13:20:27.525712Z",
          "iopub.execute_input": "2024-11-13T13:20:27.526259Z",
          "iopub.status.idle": "2024-11-13T13:20:27.537023Z",
          "shell.execute_reply.started": "2024-11-13T13:20:27.526219Z",
          "shell.execute_reply": "2024-11-13T13:20:27.535788Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(15, 15))\n",
        "plt.imshow(sample_image)\n",
        "plt.title('Sample image before tiling')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "4-L5KABIwnGg",
        "execution": {
          "iopub.status.busy": "2024-11-13T13:20:27.538421Z",
          "iopub.execute_input": "2024-11-13T13:20:27.538834Z",
          "iopub.status.idle": "2024-11-13T13:20:29.067104Z",
          "shell.execute_reply.started": "2024-11-13T13:20:27.538798Z",
          "shell.execute_reply": "2024-11-13T13:20:29.065602Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "We are going to usea a tile of size $272\\times272$ In order to apply a seamless semantic segmentation."
      ],
      "metadata": {
        "id": "n3tTKhpLxbhy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def crop_image(image, tile_shape=(272, 272), show_results=True) -> np.ndarray:\n",
        "  \"\"\"\n",
        "  Parameters:\n",
        "\n",
        "    - image=np.ndarray. Representation of image as a numpy tensor, must have 3 dimensions.\n",
        "    - tile_shape= tuple. size of tile dimensions. Its values must be divisors of the ones of the original image according with axes. Each cropped image will have this size\n",
        "    - show_results=bool. Boolean selection for visualization purposes.\n",
        "\n",
        "  Output: List of cropped images as a Numpy ndarray\n",
        "  \"\"\"\n",
        "\n",
        "  image_shape = image.shape\n",
        "  n_rows = image_shape[0] // tile_shape[0]\n",
        "  n_cols = image_shape[1] // tile_shape[1]\n",
        "\n",
        "  sub_images = []\n",
        "  if show_results==True:\n",
        "    fig, ax = plt.subplots(nrows=n_rows, ncols=n_cols, figsize=(15, 15))\n",
        "\n",
        "  for i in range(n_rows):\n",
        "    for j in range(n_cols):\n",
        "      sub_image = sample_image[i*272:(1+i)*272,j*272:(1+j)*272,:]\n",
        "      sub_images.append(sub_image)\n",
        "      if show_results==True:\n",
        "        ax[i, j].imshow(sub_image)\n",
        "        ax[i, j].axis('off')\n",
        "\n",
        "  if show_results==True:\n",
        "    fig.suptitle('Cropped Images')\n",
        "    plt.show()\n",
        "\n",
        "  sub_images = np.array(sub_images)\n",
        "  return sub_images"
      ],
      "metadata": {
        "id": "GSBTWxBDxuT3",
        "execution": {
          "iopub.status.busy": "2024-11-13T13:20:56.670526Z",
          "iopub.execute_input": "2024-11-13T13:20:56.671023Z",
          "iopub.status.idle": "2024-11-13T13:20:56.684199Z",
          "shell.execute_reply.started": "2024-11-13T13:20:56.670982Z",
          "shell.execute_reply": "2024-11-13T13:20:56.682931Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sub_images = crop_image(sample_image)"
      ],
      "metadata": {
        "id": "mAdCOJTiCevc",
        "execution": {
          "iopub.status.busy": "2024-11-13T13:21:00.742095Z",
          "iopub.execute_input": "2024-11-13T13:21:00.742550Z",
          "iopub.status.idle": "2024-11-13T13:21:05.490691Z",
          "shell.execute_reply.started": "2024-11-13T13:21:00.742514Z",
          "shell.execute_reply": "2024-11-13T13:21:05.489386Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(10, 10))\n",
        "plt.title('Example sub Image')\n",
        "plt.axis('off')\n",
        "plt.imshow(sub_images[0])\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "RWmfbWc8zJV2",
        "execution": {
          "iopub.status.busy": "2024-11-13T13:21:05.493325Z",
          "iopub.execute_input": "2024-11-13T13:21:05.493755Z",
          "iopub.status.idle": "2024-11-13T13:21:05.868199Z",
          "shell.execute_reply.started": "2024-11-13T13:21:05.493706Z",
          "shell.execute_reply": "2024-11-13T13:21:05.866653Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def re_crop_image(cropped_images, image_shape=(2448, 2448, 3), show_results=True)->np.ndarray:\n",
        "  \"\"\"\n",
        "  Parameters:\n",
        "    - cropped_images=np.ndarray: Set of sub images as a numpy tensor or list of numpy 3D arrays.\n",
        "    - image_shape=tuple. Desired output image shape.\n",
        "    - show_results=bool. Boolean selection for visualization purposes.\n",
        "  Output: new_image=np.ndarray. Image cropped as np array.\n",
        "  \"\"\"\n",
        "  new_image = np.zeros((2448, 2448, 3))\n",
        "  tile_shape = cropped_images[0].shape\n",
        "  cropped_images = list(cropped_images)\n",
        "  n_rows = image_shape[0] // tile_shape[0]\n",
        "  n_cols = image_shape[1] // tile_shape[1]\n",
        "  for i in range(n_rows):\n",
        "    for j in range(n_cols):\n",
        "      sub_image = cropped_images[i*n_rows+j]\n",
        "      new_image[i*272:(1+i)*272,j*272:(1+j)*272,:] = sub_image / 255\n",
        "\n",
        "  if show_results:\n",
        "    plt.figure(figsize=(15, 15))\n",
        "    plt.axis('off')\n",
        "    plt.title('Re-Cropped Image')\n",
        "    plt.imshow(new_image)\n",
        "  return new_image"
      ],
      "metadata": {
        "id": "-2Ty51aL9rPu",
        "execution": {
          "iopub.status.busy": "2024-11-13T13:21:05.870091Z",
          "iopub.execute_input": "2024-11-13T13:21:05.870582Z",
          "iopub.status.idle": "2024-11-13T13:21:05.885426Z",
          "shell.execute_reply.started": "2024-11-13T13:21:05.870535Z",
          "shell.execute_reply": "2024-11-13T13:21:05.883807Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "re_cropped_image = re_crop_image(sub_images)"
      ],
      "metadata": {
        "id": "6-00qya4EbhV",
        "execution": {
          "iopub.status.busy": "2024-11-13T13:21:05.888070Z",
          "iopub.execute_input": "2024-11-13T13:21:05.888460Z",
          "iopub.status.idle": "2024-11-13T13:21:08.213391Z",
          "shell.execute_reply.started": "2024-11-13T13:21:05.888425Z",
          "shell.execute_reply": "2024-11-13T13:21:08.212013Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sub_images.shape"
      ],
      "metadata": {
        "id": "xTWDspjN1rjI",
        "execution": {
          "iopub.status.busy": "2024-11-13T13:21:08.214981Z",
          "iopub.execute_input": "2024-11-13T13:21:08.215347Z",
          "iopub.status.idle": "2024-11-13T13:21:08.223473Z",
          "shell.execute_reply.started": "2024-11-13T13:21:08.215315Z",
          "shell.execute_reply": "2024-11-13T13:21:08.222244Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Prepare data to train"
      ],
      "metadata": {
        "id": "da-f6ze0wH3P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_image_paths, train_mask_paths, val_image_paths, val_mask_paths = (train_data_paths['sat_image_path'], train_data_paths['mask_path'], test_data_paths['sat_image_path'], test_data_paths['mask_path'])"
      ],
      "metadata": {
        "id": "2hR-qQKNeEjP",
        "execution": {
          "iopub.status.busy": "2024-11-13T13:21:26.552456Z",
          "iopub.execute_input": "2024-11-13T13:21:26.553771Z",
          "iopub.status.idle": "2024-11-13T13:21:26.560508Z",
          "shell.execute_reply.started": "2024-11-13T13:21:26.553713Z",
          "shell.execute_reply": "2024-11-13T13:21:26.559502Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(train_image_paths, train_mask_paths, test_size=0.2, random_state=42)"
      ],
      "metadata": {
        "id": "tEs8qF0km0KI",
        "execution": {
          "iopub.status.busy": "2024-11-13T13:21:26.980073Z",
          "iopub.execute_input": "2024-11-13T13:21:26.980522Z",
          "iopub.status.idle": "2024-11-13T13:21:26.989131Z",
          "shell.execute_reply.started": "2024-11-13T13:21:26.980485Z",
          "shell.execute_reply": "2024-11-13T13:21:26.987877Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train"
      ],
      "metadata": {
        "id": "WukCW8S5qJ30",
        "execution": {
          "iopub.status.busy": "2024-11-13T13:21:27.286147Z",
          "iopub.execute_input": "2024-11-13T13:21:27.287030Z",
          "iopub.status.idle": "2024-11-13T13:21:27.296535Z",
          "shell.execute_reply.started": "2024-11-13T13:21:27.286992Z",
          "shell.execute_reply": "2024-11-13T13:21:27.295267Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_test"
      ],
      "metadata": {
        "id": "WEIGK5GVv0VT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Reindexing\n",
        "\n",
        "X_train = pd.Series(X_train.tolist())\n",
        "X_test = pd.Series(X_test.tolist())\n",
        "\n",
        "y_train = pd.Series(y_train.tolist())\n",
        "y_test = pd.Series(y_test.tolist())"
      ],
      "metadata": {
        "id": "ojab-k2Lo5n2",
        "execution": {
          "iopub.status.busy": "2024-11-13T13:21:27.585926Z",
          "iopub.execute_input": "2024-11-13T13:21:27.586386Z",
          "iopub.status.idle": "2024-11-13T13:21:27.594301Z",
          "shell.execute_reply.started": "2024-11-13T13:21:27.586348Z",
          "shell.execute_reply": "2024-11-13T13:21:27.593092Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train"
      ],
      "metadata": {
        "id": "PlDR68tPo-Qw",
        "execution": {
          "iopub.status.busy": "2024-11-13T13:21:28.033878Z",
          "iopub.execute_input": "2024-11-13T13:21:28.034993Z",
          "iopub.status.idle": "2024-11-13T13:21:28.044685Z",
          "shell.execute_reply.started": "2024-11-13T13:21:28.034948Z",
          "shell.execute_reply": "2024-11-13T13:21:28.043248Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.imshow(plt.imread(X_train[2]))\n",
        "plt.title('Example Sat Image')\n",
        "plt.axis('off')\n",
        "plt.show()"
      ],
      "metadata": {
        "id": "x8_RLrSxnelI",
        "execution": {
          "iopub.status.busy": "2024-11-13T13:21:28.456566Z",
          "iopub.execute_input": "2024-11-13T13:21:28.457417Z",
          "iopub.status.idle": "2024-11-13T13:21:29.520972Z",
          "shell.execute_reply.started": "2024-11-13T13:21:28.457373Z",
          "shell.execute_reply": "2024-11-13T13:21:29.519739Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "datagen = ImageDataGenerator(\n",
        "    rotation_range=20,\n",
        "    width_shift_range=0.2,\n",
        "    height_shift_range=0.2,\n",
        "    shear_range=0.2,\n",
        "    zoom_range=0.2,\n",
        "    horizontal_flip=True,\n",
        "    fill_mode='nearest'\n",
        ")"
      ],
      "metadata": {
        "id": "NzGEu9vZkyPV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def load_data(images_path, masks_path, shape):\n",
        "  data = {'images': [], 'masks': []}\n",
        "  progress_bar = tqdm(range(len(images_path)),\n",
        "                      desc=\"Loading Data\",\n",
        "                      total=len(images_path),\n",
        "                      bar_format=\"{desc}: {percentage:3.0f}%|\\033[32m{bar}\\033[0m| {n_fmt}/{total_fmt} [{elapsed}<{remaining}]\")\n",
        "  for i in progress_bar:\n",
        "\n",
        "    img = plt.imread(images_path[i])\n",
        "    mask = plt.imread(masks_path[i])\n",
        "\n",
        "    img = cv.resize(img, shape)\n",
        "    mask = cv.resize(mask, shape)\n",
        "\n",
        "    data['images'].append(img)\n",
        "    data['masks'].append(mask)\n",
        "\n",
        "  data['images'] = np.array(data['images'])\n",
        "  data['masks'] = np.array(data['masks'])\n",
        "\n",
        "  return data"
      ],
      "metadata": {
        "id": "zIlsyhwmZ878",
        "execution": {
          "iopub.status.busy": "2024-11-13T13:21:29.523289Z",
          "iopub.execute_input": "2024-11-13T13:21:29.523743Z",
          "iopub.status.idle": "2024-11-13T13:21:29.535490Z",
          "shell.execute_reply.started": "2024-11-13T13:21:29.523702Z",
          "shell.execute_reply": "2024-11-13T13:21:29.534089Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data = load_data(images_path=X_train, masks_path=y_train,\n",
        "                       shape=(256, 256)\n",
        "                       )\n",
        "test_data = load_data(images_path=X_test, masks_path=y_test,\n",
        "                      shape=(256, 256)\n",
        "                      )"
      ],
      "metadata": {
        "id": "bQ-RE7f4hfgy",
        "execution": {
          "iopub.status.busy": "2024-11-13T13:21:29.862397Z",
          "iopub.execute_input": "2024-11-13T13:21:29.863504Z",
          "iopub.status.idle": "2024-11-13T13:25:35.489038Z",
          "shell.execute_reply.started": "2024-11-13T13:21:29.863459Z",
          "shell.execute_reply": "2024-11-13T13:25:35.487764Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data['masks'].shape"
      ],
      "metadata": {
        "id": "pdcQBVU2k6_3",
        "execution": {
          "iopub.status.busy": "2024-11-13T13:31:32.142184Z",
          "iopub.execute_input": "2024-11-13T13:31:32.142747Z",
          "iopub.status.idle": "2024-11-13T13:31:32.151171Z",
          "shell.execute_reply.started": "2024-11-13T13:31:32.142706Z",
          "shell.execute_reply": "2024-11-13T13:31:32.149884Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(X_train)"
      ],
      "metadata": {
        "id": "sEOrMA3PyHDY",
        "execution": {
          "iopub.status.busy": "2024-11-13T13:31:32.463409Z",
          "iopub.execute_input": "2024-11-13T13:31:32.463842Z",
          "iopub.status.idle": "2024-11-13T13:31:32.472189Z",
          "shell.execute_reply.started": "2024-11-13T13:31:32.463807Z",
          "shell.execute_reply": "2024-11-13T13:31:32.470826Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "train_data['masks'][0]"
      ],
      "metadata": {
        "id": "enbgcSu-BR1N",
        "execution": {
          "iopub.status.busy": "2024-11-13T13:31:32.805163Z",
          "iopub.execute_input": "2024-11-13T13:31:32.805584Z",
          "iopub.status.idle": "2024-11-13T13:31:32.817209Z",
          "shell.execute_reply.started": "2024-11-13T13:31:32.805552Z",
          "shell.execute_reply": "2024-11-13T13:31:32.815687Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "0wWLhYHktN8n"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training"
      ],
      "metadata": {
        "id": "7dze0c_wwo9h"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## U-Net"
      ],
      "metadata": {
        "id": "BqQ5l3nuNksC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# def create_conv_block(input_tensor, num_filters):\n",
        "#   \"\"\"\n",
        "#     Helps us to create the convolutional blocks\n",
        "#   \"\"\"\n",
        "#   # First Conv layer\n",
        "#   x = tf.keras.layers.Conv2D(filters=num_filters, kernel_size=(3, 3), kernel_initializer='he_normal', padding='same')(input_tensor)\n",
        "#   x = tf.keras.layers.BatchNormalization()(x)\n",
        "#   x = tf.keras.layers.Activation('relu')(x)\n",
        "#   # Second Conv layer\n",
        "\n",
        "#   x = tf.keras.layers.Conv2D(filters=num_filters, kernel_size=(3, 3), kernel_initializer='he_normal', padding='same')(x)\n",
        "#   x = tf.keras.layers.BatchNormalization()(x)\n",
        "#   x = tf.keras.layers.Activation('relu')(x)\n",
        "\n",
        "#   return x\n",
        "\n",
        "def create_conv_block(input_tensor, num_filters):\n",
        "    \"\"\"\n",
        "    Enhanced convolutional block with residual connection and advanced activation\n",
        "    \"\"\"\n",
        "    # First Conv layer with residual connection\n",
        "    x = tf.keras.layers.Conv2D(filters=num_filters, kernel_size=(3, 3),\n",
        "                                kernel_initializer='he_normal', padding='same')(input_tensor)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "    x = tf.keras.layers.Activation('swish')(x)  # Replace ReLU with Swish activation\n",
        "\n",
        "    # Second Conv layer\n",
        "    x = tf.keras.layers.Conv2D(filters=num_filters, kernel_size=(3, 3),\n",
        "                                kernel_initializer='he_normal', padding='same')(x)\n",
        "    x = tf.keras.layers.BatchNormalization()(x)\n",
        "\n",
        "    # Residual connection\n",
        "    if input_tensor.shape[-1] != num_filters:\n",
        "        shortcut = tf.keras.layers.Conv2D(num_filters, (1, 1), padding='same')(input_tensor)\n",
        "    else:\n",
        "        shortcut = input_tensor\n",
        "\n",
        "    # Combine residual connection\n",
        "    x = tf.keras.layers.Add()([x, shortcut])\n",
        "    x = tf.keras.layers.Activation('swish')(x)\n",
        "\n",
        "    return x"
      ],
      "metadata": {
        "id": "r2IawCk_rnzJ",
        "execution": {
          "iopub.status.busy": "2024-11-13T13:31:34.135067Z",
          "iopub.execute_input": "2024-11-13T13:31:34.136648Z",
          "iopub.status.idle": "2024-11-13T13:31:34.146798Z",
          "shell.execute_reply.started": "2024-11-13T13:31:34.136589Z",
          "shell.execute_reply": "2024-11-13T13:31:34.145551Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# def create_unet(input_shape, num_filters=16, dropout=0.1):\n",
        "#   \"\"\"\n",
        "#     Creates the U-Net Architecture\n",
        "#   \"\"\"\n",
        "#   # Encoder\n",
        "#     # First conv block\n",
        "#   c1 = create_conv_block(input_shape, num_filters * 1)\n",
        "#   p1 = tf.keras.layers.MaxPool2D((2, 2))(c1)\n",
        "#   p1 = tf.keras.layers.Dropout(dropout)(p1)\n",
        "#     # Second conv block\n",
        "#   c2 = create_conv_block(p1, num_filters * 2)\n",
        "#   p2 = tf.keras.layers.MaxPool2D((2, 2))(c2)\n",
        "#   p2 = tf.keras.layers.Dropout(dropout)(p2)\n",
        "#     # Third conv block\n",
        "#   c3 = create_conv_block(p2, num_filters * 4)\n",
        "#   p3 = tf.keras.layers.MaxPool2D((2, 2))(c3)\n",
        "#   p3 = tf.keras.layers.Dropout(dropout)(p3)\n",
        "#     # Fourth conv block\n",
        "#   c4 = create_conv_block(p3, num_filters * 8)\n",
        "#   p4 = tf.keras.layers.MaxPool2D((2, 2))(c4)\n",
        "#   p4 = tf.keras.layers.Dropout(dropout)(p4)\n",
        "\n",
        "#   c5 = create_conv_block(p4, num_filters*16)\n",
        "\n",
        "#   # Decoder\n",
        "#     # First block\n",
        "#   u6 = tf.keras.layers.Conv2DTranspose(num_filters*8, (3, 3), strides=(2, 2), padding='same')(c5)\n",
        "#   u6 = tf.keras.layers.concatenate([u6, c4])\n",
        "#   u6 = tf.keras.layers.Dropout(dropout)(u6)\n",
        "#   c6 = create_conv_block(u6, num_filters*8)\n",
        "#     # Second block\n",
        "#   u7 = tf.keras.layers.Conv2DTranspose(num_filters*4, (3, 3), strides=(2, 2), padding='same')(c6)\n",
        "#   u7 = tf.keras.layers.concatenate([u7, c3])\n",
        "#   u7 = tf.keras.layers.Dropout(dropout)(u7)\n",
        "#   c7 = create_conv_block(u7, num_filters*4)\n",
        "#     #Third block\n",
        "#   u8 = tf.keras.layers.Conv2DTranspose(num_filters*2, (3, 3), strides=(2, 2), padding='same')(c7)\n",
        "#   u8 = tf.keras.layers.concatenate([u8, c2])\n",
        "#   u8 = tf.keras.layers.Dropout(dropout)(u8)\n",
        "#   c8 = create_conv_block(u8, num_filters*2)\n",
        "#     # Fourth block\n",
        "#   u9 = tf.keras.layers.Conv2DTranspose(num_filters*1, (3, 3), strides=(2, 2), padding='same')(c8)\n",
        "#   u9 = tf.keras.layers.concatenate([u9, c1])\n",
        "#   c9 = tf.keras.layers.Dropout(dropout)(u9)\n",
        "\n",
        "#   output = tf.keras.layers.Conv2D(3, (1, 1), activation='sigmoid')(c9)\n",
        "#   model = tf.keras.Model(inputs = [input_shape], outputs=[output])\n",
        "\n",
        "#   return model\n",
        "\n",
        "def create_unet(input_shape, num_filters=16, dropout=0.2):\n",
        "    \"\"\"\n",
        "    Enhanced U-Net Architecture with Multi-Scale Feature Fusion\n",
        "    \"\"\"\n",
        "    # Encoder\n",
        "    # First conv block with multi-scale feature preparation\n",
        "    c1 = create_conv_block(input_shape, num_filters * 1)\n",
        "    p1 = tf.keras.layers.MaxPool2D((2, 2))(c1)\n",
        "    p1 = tf.keras.layers.SpatialDropout2D(dropout)(p1)\n",
        "\n",
        "    # Second conv block with multi-scale feature fusion\n",
        "    c2 = create_conv_block(p1, num_filters * 2)\n",
        "    # Resize c1 before concatenation to match c2's spatial dimensions\n",
        "    c1_resized = tf.keras.layers.Resizing(c2.shape[1], c2.shape[2])(c1)\n",
        "    c2_fused = tf.keras.layers.concatenate([c1_resized, c2])  # Multi-scale feature fusion\n",
        "    p2 = tf.keras.layers.MaxPool2D((2, 2))(c2)\n",
        "    p2 = tf.keras.layers.SpatialDropout2D(dropout)(p2)\n",
        "\n",
        "    # Third conv block\n",
        "    c3 = create_conv_block(p2, num_filters * 4)\n",
        "    # Resize c2_fused before concatenation to match c3's spatial dimensions\n",
        "    c2_fused_resized = tf.keras.layers.Resizing(c3.shape[1], c3.shape[2])(c2_fused)\n",
        "    c3_fused = tf.keras.layers.concatenate([c2_fused_resized, c3])\n",
        "    p3 = tf.keras.layers.MaxPool2D((2, 2))(c3)\n",
        "    p3 = tf.keras.layers.SpatialDropout2D(dropout)(p3)\n",
        "\n",
        "    # Fourth conv block\n",
        "    c4 = create_conv_block(p3, num_filters * 8)\n",
        "    # Resize c3_fused before concatenation to match c4's spatial dimensions\n",
        "    c3_fused_resized = tf.keras.layers.Resizing(c4.shape[1], c4.shape[2])(c3_fused)\n",
        "    c4_fused = tf.keras.layers.concatenate([c3_fused_resized, c4])\n",
        "    p4 = tf.keras.layers.MaxPool2D((2, 2))(c4)\n",
        "    p4 = tf.keras.layers.SpatialDropout2D(dropout)(p4)\n",
        "\n",
        "    # Bottleneck\n",
        "    c5 = create_conv_block(p4, num_filters*16)\n",
        "\n",
        "    # Decoder with enhanced feature fusion\n",
        "    # First block\n",
        "    u6 = tf.keras.layers.Conv2DTranspose(num_filters*8, (3, 3), strides=(2, 2), padding='same')(c5)\n",
        "    # Resize c4_fused before concatenation to match u6's spatial dimensions\n",
        "    c4_fused_resized = tf.keras.layers.Resizing(u6.shape[1], u6.shape[2])(c4_fused)\n",
        "    u6 = tf.keras.layers.concatenate([u6, c4_fused_resized])\n",
        "    u6 = tf.keras.layers.SpatialDropout2D(dropout)(u6)\n",
        "    c6 = create_conv_block(u6, num_filters*8)\n",
        "\n",
        "    # Second block\n",
        "    u7 = tf.keras.layers.Conv2DTranspose(num_filters*4, (3, 3), strides=(2, 2), padding='same')(c6)\n",
        "    # Resize c3_fused before concatenation to match u7's spatial dimensions\n",
        "    c3_fused_resized = tf.keras.layers.Resizing(u7.shape[1], u7.shape[2])(c3_fused)\n",
        "    u7 = tf.keras.layers.concatenate([u7, c3_fused_resized])\n",
        "    u7 = tf.keras.layers.SpatialDropout2D(dropout)(u7)\n",
        "    c7 = create_conv_block(u7, num_filters*4)\n",
        "\n",
        "    # Third block\n",
        "    u8 = tf.keras.layers.Conv2DTranspose(num_filters*2, (3, 3), strides=(2, 2), padding='same')(c7)\n",
        "    # Resize c2_fused before concatenation to match u8's spatial dimensions\n",
        "    c2_fused_resized = tf.keras.layers.Resizing(u8.shape[1], u8.shape[2])(c2_fused)\n",
        "    u8 = tf.keras.layers.concatenate([u8, c2_fused_resized])\n",
        "    u8 = tf.keras.layers.SpatialDropout2D(dropout)(u8)\n",
        "    c8 = create_conv_block(u8, num_filters*2)\n",
        "\n",
        "    # Fourth block\n",
        "    u9 = tf.keras.layers.Conv2DTranspose(num_filters*1, (3, 3), strides=(2, 2), padding='same')(c8)\n",
        "    u9 = tf.keras.layers.concatenate([u9, c1])  # c1 shape is already compatible\n",
        "    c9 = tf.keras.layers.SpatialDropout2D(dropout)(u9)\n",
        "\n",
        "    # Enhanced output layer with multiple convolutions\n",
        "    output = tf.keras.layers.Conv2D(64, (3, 3), padding='same', activation='swish')(c9)\n",
        "    output = tf.keras.layers.BatchNormalization()(output)\n",
        "    output = tf.keras.layers.Conv2D(32, (3, 3), padding='same', activation='swish')(output)\n",
        "    output = tf.keras.layers.Conv2D(3, (1, 1), activation='sigmoid')(output)\n",
        "\n",
        "    model = tf.keras.Model(inputs=[input_shape], outputs=[output])\n",
        "\n",
        "    return model"
      ],
      "metadata": {
        "id": "uYsH4rLkv-CS",
        "execution": {
          "iopub.status.busy": "2024-11-13T13:31:34.548052Z",
          "iopub.execute_input": "2024-11-13T13:31:34.548511Z",
          "iopub.status.idle": "2024-11-13T13:31:34.571647Z",
          "shell.execute_reply.started": "2024-11-13T13:31:34.548474Z",
          "shell.execute_reply": "2024-11-13T13:31:34.570459Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_shape = train_data['masks'][0].shape;\n",
        "inputs = tf.keras.layers.Input(input_shape);\n",
        "\n",
        "model = create_unet(inputs, num_filters=32);"
      ],
      "metadata": {
        "id": "1H-eDf0oSfuT",
        "execution": {
          "iopub.status.busy": "2024-11-13T13:31:35.035483Z",
          "iopub.execute_input": "2024-11-13T13:31:35.035948Z",
          "iopub.status.idle": "2024-11-13T13:31:35.715401Z",
          "shell.execute_reply.started": "2024-11-13T13:31:35.035887Z",
          "shell.execute_reply": "2024-11-13T13:31:35.714069Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(inputs)"
      ],
      "metadata": {
        "id": "88wX1M4Y2RY4",
        "execution": {
          "iopub.status.busy": "2024-11-13T13:31:35.717646Z",
          "iopub.execute_input": "2024-11-13T13:31:35.718663Z",
          "iopub.status.idle": "2024-11-13T13:31:35.724433Z",
          "shell.execute_reply.started": "2024-11-13T13:31:35.718615Z",
          "shell.execute_reply": "2024-11-13T13:31:35.723079Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "early = EarlyStopping(patience=20)\n",
        "print(input_shape)"
      ],
      "metadata": {
        "id": "gbd3kpul03ye",
        "execution": {
          "iopub.status.busy": "2024-11-13T13:31:36.053976Z",
          "iopub.execute_input": "2024-11-13T13:31:36.055073Z",
          "iopub.status.idle": "2024-11-13T13:31:36.061622Z",
          "shell.execute_reply.started": "2024-11-13T13:31:36.055028Z",
          "shell.execute_reply": "2024-11-13T13:31:36.060284Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(optimizer='Adam', loss='binary_crossentropy', metrics=['accuracy'])"
      ],
      "metadata": {
        "id": "7Mt6J7nfL0zR",
        "execution": {
          "iopub.status.busy": "2024-11-13T13:31:39.743743Z",
          "iopub.execute_input": "2024-11-13T13:31:39.744171Z",
          "iopub.status.idle": "2024-11-13T13:31:39.765095Z",
          "shell.execute_reply.started": "2024-11-13T13:31:39.744137Z",
          "shell.execute_reply": "2024-11-13T13:31:39.763773Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.summary()"
      ],
      "metadata": {
        "id": "MOoYk8RLF0g1",
        "execution": {
          "iopub.status.busy": "2024-11-13T13:31:40.119117Z",
          "iopub.execute_input": "2024-11-13T13:31:40.120105Z",
          "iopub.status.idle": "2024-11-13T13:31:40.137848Z",
          "shell.execute_reply.started": "2024-11-13T13:31:40.120061Z",
          "shell.execute_reply": "2024-11-13T13:31:40.136452Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tf.keras.utils.plot_model(model, show_shapes=True)"
      ],
      "metadata": {
        "id": "WUHpHUpGTqS9",
        "execution": {
          "iopub.status.busy": "2024-11-13T13:31:40.516504Z",
          "iopub.execute_input": "2024-11-13T13:31:40.516949Z",
          "iopub.status.idle": "2024-11-13T13:31:42.981023Z",
          "shell.execute_reply.started": "2024-11-13T13:31:40.516913Z",
          "shell.execute_reply": "2024-11-13T13:31:42.979336Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "checkpoint = tf.keras.callbacks.ModelCheckpoint('best.keras', monitor='accuracy', save_best_only=True, verbose=1)"
      ],
      "metadata": {
        "id": "PFq0Cq_Jh05o",
        "execution": {
          "iopub.status.busy": "2024-11-13T13:31:42.983539Z",
          "iopub.execute_input": "2024-11-13T13:31:42.984001Z",
          "iopub.status.idle": "2024-11-13T13:31:42.991059Z",
          "shell.execute_reply.started": "2024-11-13T13:31:42.983955Z",
          "shell.execute_reply": "2024-11-13T13:31:42.989625Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_history = model.fit(train_data['images'], train_data['masks'],\n",
        "                          epochs=100,\n",
        "                          verbose=0,\n",
        "                          batch_size=16,\n",
        "                          callbacks=[checkpoint, TqdmCallback(verbose=1), early]\n",
        "                        )"
      ],
      "metadata": {
        "id": "U9H2iCwqiP1T",
        "execution": {
          "iopub.status.busy": "2024-11-13T13:31:43.431269Z",
          "iopub.execute_input": "2024-11-13T13:31:43.431713Z",
          "iopub.status.idle": "2024-11-13T14:02:13.726132Z",
          "shell.execute_reply.started": "2024-11-13T13:31:43.431676Z",
          "shell.execute_reply": "2024-11-13T14:02:13.724171Z"
        },
        "trusted": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# model = load_model('/content/drive/MyDrive/Satellite Dataset/model.hdf5')\n",
        "model.save('/content/drive/MyDrive/Satellite Dataset/eighty.keras')"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-11-13T14:04:50.225647Z",
          "iopub.execute_input": "2024-11-13T14:04:50.226133Z",
          "iopub.status.idle": "2024-11-13T14:04:52.443155Z",
          "shell.execute_reply.started": "2024-11-13T14:04:50.226093Z",
          "shell.execute_reply": "2024-11-13T14:04:52.441811Z"
        },
        "trusted": true,
        "id": "bkZ07elv1ZNE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = load_model('/content/drive/MyDrive/Satellite Dataset/eighty.keras')"
      ],
      "metadata": {
        "id": "NPso-LFT4PXL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-11-13T14:04:52.446026Z",
          "iopub.execute_input": "2024-11-13T14:04:52.447198Z",
          "iopub.status.idle": "2024-11-13T14:04:52.455148Z",
          "shell.execute_reply.started": "2024-11-13T14:04:52.447146Z",
          "shell.execute_reply": "2024-11-13T14:04:52.453873Z"
        },
        "trusted": true,
        "id": "NzaOs_7o1ZNE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def predict_test_samples(val_map, model):\n",
        "    # Get the first 5 images and masks\n",
        "    imgs = val_map['images'][:25]\n",
        "    masks = val_map['masks'][:25]\n",
        "\n",
        "    # Predict masks for these 25 images\n",
        "    predicted_masks = model.predict(imgs)\n",
        "\n",
        "    return imgs, predicted_masks, masks\n"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-11-13T14:08:19.950543Z",
          "iopub.execute_input": "2024-11-13T14:08:19.951073Z",
          "iopub.status.idle": "2024-11-13T14:08:19.958956Z",
          "shell.execute_reply.started": "2024-11-13T14:08:19.951029Z",
          "shell.execute_reply": "2024-11-13T14:08:19.957427Z"
        },
        "trusted": true,
        "id": "LQtlC8-21ZNE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "imgs, predicted_masks, masks = predict_test_samples(test_data, model);"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-11-13T14:08:35.375938Z",
          "iopub.execute_input": "2024-11-13T14:08:35.376395Z",
          "iopub.status.idle": "2024-11-13T14:08:36.390930Z",
          "shell.execute_reply.started": "2024-11-13T14:08:35.376358Z",
          "shell.execute_reply": "2024-11-13T14:08:36.389466Z"
        },
        "trusted": true,
        "id": "qUlnBEva1ZNE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def plot_predictions(image, mask_pred, ground_truth_mask):\n",
        "  fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(30, 10))\n",
        "\n",
        "  ax[0].imshow(image)\n",
        "  ax[0].set_title('Original satellital image')\n",
        "  ax[0].axis('off')\n",
        "\n",
        "  ax[1].imshow(mask_pred)\n",
        "  ax[1].set_title('Predicted Mask')\n",
        "  ax[1].axis('off')\n",
        "\n",
        "  ax[2].imshow(ground_truth_mask)\n",
        "  ax[2].set_title('Ground truth Mask')\n",
        "  ax[2].axis('off')\n",
        "\n",
        "  plt.show()"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-11-13T14:08:39.247056Z",
          "iopub.execute_input": "2024-11-13T14:08:39.247531Z",
          "iopub.status.idle": "2024-11-13T14:08:39.258348Z",
          "shell.execute_reply.started": "2024-11-13T14:08:39.247493Z",
          "shell.execute_reply": "2024-11-13T14:08:39.256698Z"
        },
        "trusted": true,
        "id": "ho0BQPI_1ZNE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(5):\n",
        "  plot_predictions(imgs[i], predicted_masks[i], masks[i])"
      ],
      "metadata": {
        "execution": {
          "iopub.status.busy": "2024-11-13T14:08:44.388049Z",
          "iopub.execute_input": "2024-11-13T14:08:44.388512Z",
          "iopub.status.idle": "2024-11-13T14:08:46.622124Z",
          "shell.execute_reply.started": "2024-11-13T14:08:44.388473Z",
          "shell.execute_reply": "2024-11-13T14:08:46.620918Z"
        },
        "trusted": true,
        "id": "2M9jqHm51ZNE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inference"
      ],
      "metadata": {
        "id": "qUvueUF-JsZB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# import cv2\n",
        "# import numpy as np\n",
        "# import pandas as pd\n",
        "# from google.colab.patches import cv2_imshow\n",
        "\n",
        "# def detect_class_color_portions(image_path):\n",
        "#     \"\"\"Detects portions of land classes in an image based on RGB colors and calculates their percentage in the image.\"\"\"\n",
        "\n",
        "#     class_colors = {\n",
        "#         \"urban_land\": [0, 255, 255],\n",
        "#         \"agriculture_land\": [255, 255, 0],\n",
        "#         \"rangeland\": [255, 0, 255],\n",
        "#         \"forest_land\": [0, 255, 0],\n",
        "#         \"water\": [0, 0, 255],\n",
        "#         \"barren_land\": [255, 255, 255],\n",
        "#         \"Unknown\": [0, 0, 0]\n",
        "#     }\n",
        "\n",
        "#     image = cv2.imread(image_path)\n",
        "#     if image is None:\n",
        "#         print(f\"Error: Image {image_path} not loaded properly.\")\n",
        "#         return None\n",
        "\n",
        "#     hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
        "\n",
        "#     color_ranges = {\n",
        "#         \"urban_land\": [np.array([20, 100, 100]), np.array([40, 255, 255])],\n",
        "#         \"agriculture_land\": [np.array([80, 100, 100]), np.array([100, 255, 255])],\n",
        "#         \"rangeland\": [np.array([140, 100, 100]), np.array([170, 255, 255])],\n",
        "#         \"forest_land\": [np.array([36, 50, 50]), np.array([86, 255, 255])],\n",
        "#         \"water\": [np.array([100, 50, 50]), np.array([130, 255, 255])],\n",
        "#         \"barren_land\": [np.array([0, 0, 255]), np.array([180, 255, 255])],\n",
        "#         \"unknown\": [np.array([0, 0, 0]), np.array([180, 255, 50])]\n",
        "#     }\n",
        "\n",
        "#     total_pixels = image.shape[0] * image.shape[1]\n",
        "#     class_percentages = {}\n",
        "\n",
        "#     for class_name, (lower, upper) in color_ranges.items():\n",
        "#         mask = cv2.inRange(hsv, lower, upper)\n",
        "#         class_pixels = cv2.countNonZero(mask)\n",
        "#         class_percentage = (class_pixels / total_pixels) * 100\n",
        "#         class_percentages[class_name] = class_percentage\n",
        "#     return pd.DataFrame.from_dict(class_percentages, orient='index', columns=['Percentage'])\n",
        "\n",
        "# def compare_images_with_display(image_path1, image_path2):\n",
        "#     df1 = detect_class_color_portions(image_path1)\n",
        "#     df2 = detect_class_color_portions(image_path2)\n",
        "\n",
        "#     if df1 is None or df2 is None:\n",
        "#         print(\"Error: One or both images failed to load or process.\")\n",
        "#         return None\n",
        "\n",
        "#     # Compute the difference as image1 - image2\n",
        "#     df_comparison = pd.merge(df1, df2, left_index=True, right_index=True, suffixes=('_image1', '_image2'))\n",
        "#     df_comparison['Difference'] = df_comparison['Percentage_image1'] - df_comparison['Percentage_image2']\n",
        "\n",
        "#     # Generate the textual statements\n",
        "#     statements = []\n",
        "#     for class_name, row in df_comparison.iterrows():\n",
        "#         percentage1 = row['Percentage_image1']\n",
        "#         percentage2 = row['Percentage_image2']\n",
        "#         difference = row['Difference']\n",
        "#         change_type = \"increased\" if difference > 0 else \"reduced\"\n",
        "\n",
        "#         if difference != 0:  # Only include classes with differences\n",
        "#             statements.append(\n",
        "#                 f\"{class_name.replace('_', ' ').title()} has been {change_type} from {percentage1:.2f}% to {percentage2:.2f}% \"\n",
        "#                 f\"({abs(difference):.2f}% change).\"\n",
        "#             )\n",
        "\n",
        "#     # Display images side by side\n",
        "#     img1 = cv2.imread(image_path1)\n",
        "#     img2 = cv2.imread(image_path2)\n",
        "\n",
        "#     if img1 is None or img2 is None:\n",
        "#         print(\"Error: One or both images failed to load properly.\")\n",
        "#         return None\n",
        "\n",
        "#     # Resize images to the same height for better comparison\n",
        "#     height = min(img1.shape[0], img2.shape[0])\n",
        "#     img1 = cv2.resize(img1, (int(img1.shape[1] * height / img1.shape[0]), height))\n",
        "#     img2 = cv2.resize(img2, (int(img2.shape[1] * height / img2.shape[0]), height))\n",
        "\n",
        "#     # Combine images side by side\n",
        "#     combined_image = np.hstack((img1, img2))\n",
        "\n",
        "#     # print(\"Varitions of the Areas through Satellite Images\")\n",
        "#     cv2_imshow(combined_image)\n",
        "\n",
        "#     # Generate textual change summary\n",
        "#     print(\"Change Summary:\\n\")\n",
        "#     for statement in statements:\n",
        "#         print(statement)\n",
        "\n",
        "#     return df_comparison\n",
        "\n",
        "\n",
        "model = load_model('/content/drive/MyDrive/Satellite Dataset/eighty.keras')\n",
        "\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from google.colab.patches import cv2_imshow\n",
        "\n",
        "def detect_class_color_portions(image_path):\n",
        "    \"\"\"Detects portions of land classes in an image based on RGB colors and calculates their percentage in the image.\"\"\"\n",
        "    class_colors = {\n",
        "        \"urban_land\": [0, 255, 255],\n",
        "        \"agriculture_land\": [255, 255, 0],\n",
        "        \"rangeland\": [255, 0, 255],\n",
        "        \"forest_land\": [0, 255, 0],\n",
        "        \"water\": [0, 0, 255],\n",
        "        \"barren_land\": [255, 255, 255],\n",
        "        \"Unknown\": [0, 0, 0]\n",
        "    }\n",
        "\n",
        "    image = cv2.imread(image_path)\n",
        "    if image is None:\n",
        "        print(f\"Error: Image {image_path} not loaded properly.\")\n",
        "        return None\n",
        "\n",
        "    hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
        "\n",
        "    color_ranges = {\n",
        "        \"urban_land\": [np.array([20, 100, 100]), np.array([40, 255, 255])],\n",
        "        \"agriculture_land\": [np.array([80, 100, 100]), np.array([100, 255, 255])],\n",
        "        \"rangeland\": [np.array([140, 100, 100]), np.array([170, 255, 255])],\n",
        "        \"forest_land\": [np.array([36, 50, 50]), np.array([86, 255, 255])],\n",
        "        \"water\": [np.array([100, 50, 50]), np.array([130, 255, 255])],\n",
        "        \"barren_land\": [np.array([0, 0, 255]), np.array([180, 255, 255])],\n",
        "        \"unknown\": [np.array([0, 0, 0]), np.array([180, 255, 50])]\n",
        "    }\n",
        "\n",
        "    total_pixels = image.shape[0] * image.shape[1]\n",
        "\n",
        "    class_percentages = {}\n",
        "    for class_name, (lower, upper) in color_ranges.items():\n",
        "        mask = cv2.inRange(hsv, lower, upper)\n",
        "        class_pixels = cv2.countNonZero(mask)\n",
        "        class_percentage = (class_pixels / total_pixels) * 100\n",
        "        class_percentages[class_name] = class_percentage\n",
        "\n",
        "    return pd.DataFrame.from_dict(class_percentages, orient='index', columns=['Percentage'])\n",
        "\n",
        "def compare_images_with_display(image_path1, image_path2):\n",
        "    df1 = detect_class_color_portions(image_path1)\n",
        "    df2 = detect_class_color_portions(image_path2)\n",
        "\n",
        "    if df1 is None or df2 is None:\n",
        "        print(\"Error: One or both images failed to load or process.\")\n",
        "        return None\n",
        "\n",
        "    # Compute the difference as image2 - image1 (to get correct change direction)\n",
        "    df_comparison = pd.merge(df1, df2, left_index=True, right_index=True, suffixes=('_image1', '_image2'))\n",
        "    df_comparison['Difference'] = df_comparison['Percentage_image2'] - df_comparison['Percentage_image1']\n",
        "\n",
        "    # Generate the textual statements\n",
        "    statements = []\n",
        "    for class_name, row in df_comparison.iterrows():\n",
        "        percentage1 = row['Percentage_image1']\n",
        "        percentage2 = row['Percentage_image2']\n",
        "        difference = row['Difference']\n",
        "\n",
        "        # Determine change type based on the actual difference\n",
        "        change_type = \"increased\" if difference > 0 else \"reduced\"\n",
        "\n",
        "        if abs(difference) > 0.01:  # Only include classes with significant changes\n",
        "            statements.append(\n",
        "                f\"{class_name.replace('_', ' ').title()} has been {change_type} from {percentage1:.2f}% to {percentage2:.2f}% \"\n",
        "                f\"({abs(difference):.2f}% change).\"\n",
        "            )\n",
        "\n",
        "    # Display images side by side\n",
        "    img1 = cv2.imread(image_path1)\n",
        "    img2 = cv2.imread(image_path2)\n",
        "\n",
        "    if img1 is None or img2 is None:\n",
        "        print(\"Error: One or both images failed to load properly.\")\n",
        "        return None\n",
        "\n",
        "    # Resize images to the same height for better comparison\n",
        "    height = min(img1.shape[0], img2.shape[0])\n",
        "    img1 = cv2.resize(img1, (int(img1.shape[1] * height / img1.shape[0]), height))\n",
        "    img2 = cv2.resize(img2, (int(img2.shape[1] * height / img2.shape[0]), height))\n",
        "\n",
        "    # Combine images side by side\n",
        "    combined_image = np.hstack((img1, img2))\n",
        "\n",
        "    cv2_imshow(combined_image)\n",
        "\n",
        "    # Generate textual change summary\n",
        "    print(\"Change Summary:\\n\")\n",
        "    for statement in statements:\n",
        "        print(statement)\n",
        "\n",
        "    return df_comparison"
      ],
      "metadata": {
        "id": "pO6lhGAn1ZNF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import cv2 as cv\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "def predict_for_image(image_path, model, mask,  target_shape=(256, 256), save_dir='/content/drive/MyDrive/Satellite Dataset/Predictions'):\n",
        "    image = plt.imread(image_path)\n",
        "    image_resized = cv.resize(image, target_shape)\n",
        "    predicted_mask = model.predict(np.expand_dims(image_resized, axis=0))[0]\n",
        "    save_prediction(image_resized, predicted_mask, save_dir, mask)\n",
        "\n",
        "    plot_predictions(image_resized, predicted_mask)\n",
        "\n",
        "def save_prediction(image, mask_pred, save_dir, mask):\n",
        "    os.makedirs(save_dir, exist_ok=True)\n",
        "\n",
        "    # Always save the mask as 'mask1.png'\n",
        "    mask_filename = os.path.join(save_dir, mask)\n",
        "\n",
        "    # Normalize mask to 0-255 and convert to uint8\n",
        "    mask_pred_normalized = (mask_pred * 255).astype(np.uint8)\n",
        "\n",
        "    # Save the mask image\n",
        "    cv.imwrite(mask_filename, mask_pred_normalized)\n",
        "    print(f\"Predicted mask saved at {mask_filename}\")\n",
        "\n",
        "def plot_predictions(image, mask_pred):\n",
        "    fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(20, 10))\n",
        "\n",
        "    ax[0].imshow(image)\n",
        "    ax[0].set_title('Original Satellite Image')\n",
        "    ax[0].axis('off')\n",
        "\n",
        "    ax[1].imshow(mask_pred, cmap='gray')\n",
        "    ax[1].set_title('Predicted Mask')\n",
        "    ax[1].axis('off')\n",
        "\n",
        "    plt.show()\n",
        "\n",
        "# Example usage\n",
        "image_path1 = '/content/259219_sat.jpg'\n",
        "image_path2 = '/content/323588_sat.jpg'\n",
        "\n",
        "predict_for_image(image_path1, model, 'mask1.png')\n",
        "predict_for_image(image_path2, model, 'mask2.png')"
      ],
      "metadata": {
        "id": "fYofBXdTuZG6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Compare the two images\n",
        "image_path1 = \"/content/drive/MyDrive/Satellite Dataset/Predictions/mask1.png\"\n",
        "image_path2 = \"/content/drive/MyDrive/Satellite Dataset/Predictions/mask2.png\"\n",
        "\n",
        "class_diff_df = compare_images_with_display(image_path1, image_path2)\n",
        "\n",
        "if class_diff_df is not None:\n",
        "    print(\"\\nAreas Difference Over the Period:\")\n",
        "    print(class_diff_df[['Difference', 'Percentage_image1', 'Percentage_image2']])"
      ],
      "metadata": {
        "id": "TvY6pIh_1ZNF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import cv2\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import load_model\n",
        "from google.colab import drive\n",
        "from google.colab.patches import cv2_imshow\n",
        "import re\n",
        "\n",
        "class SatelliteImageAnalyzer:\n",
        "    def __init__(self, model_path, predictions_dir='/content/drive/MyDrive/Satellite Dataset/Predictions'):\n",
        "        \"\"\"\n",
        "        Initialize the Satellite Image Analyzer\n",
        "\n",
        "        Args:\n",
        "            model_path (str): Path to the trained U-Net model\n",
        "            predictions_dir (str): Directory to save prediction masks\n",
        "        \"\"\"\n",
        "        # Mount Google Drive\n",
        "        drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "        # Load the pre-trained model\n",
        "        self.model = load_model(model_path)\n",
        "\n",
        "        # Set prediction directory\n",
        "        self.predictions_dir = predictions_dir\n",
        "        os.makedirs(self.predictions_dir, exist_ok=True)\n",
        "\n",
        "        # Define color ranges for land classification\n",
        "        self.color_ranges = {\n",
        "            \"urban_land\": [np.array([20, 100, 100]), np.array([40, 255, 255])],\n",
        "            \"agriculture_land\": [np.array([80, 100, 100]), np.array([100, 255, 255])],\n",
        "            \"rangeland\": [np.array([140, 100, 100]), np.array([170, 255, 255])],\n",
        "            \"forest_land\": [np.array([36, 50, 50]), np.array([86, 255, 255])],\n",
        "            \"water\": [np.array([100, 50, 50]), np.array([130, 255, 255])],\n",
        "            \"barren_land\": [np.array([0, 0, 255]), np.array([180, 255, 255])],\n",
        "            \"unknown\": [np.array([0, 0, 0]), np.array([180, 255, 50])]\n",
        "        }\n",
        "\n",
        "    def predict_mask(self, image_path, mask_name, target_shape=(256, 256)):\n",
        "        \"\"\"\n",
        "        Predict mask for a given satellite image\n",
        "\n",
        "        Args:\n",
        "            image_path (str): Path to the satellite image\n",
        "            mask_name (str): Name of the mask file to save\n",
        "            target_shape (tuple): Resize dimensions for the image\n",
        "\n",
        "        Returns:\n",
        "            numpy.ndarray: Predicted mask\n",
        "        \"\"\"\n",
        "        # Read and resize image\n",
        "        image = plt.imread(image_path)\n",
        "        image_resized = cv2.resize(image, target_shape)\n",
        "\n",
        "        # Predict mask\n",
        "        predicted_mask = self.model.predict(np.expand_dims(image_resized, axis=0))[0]\n",
        "\n",
        "        # Save prediction\n",
        "        mask_filename = os.path.join(self.predictions_dir, mask_name)\n",
        "        mask_pred_normalized = (predicted_mask * 255).astype(np.uint8)\n",
        "        cv2.imwrite(mask_filename, mask_pred_normalized)\n",
        "\n",
        "        return predicted_mask\n",
        "\n",
        "    def detect_class_color_portions(self, image_path):\n",
        "        \"\"\"\n",
        "        Detect land class portions in an image\n",
        "\n",
        "        Args:\n",
        "            image_path (str): Path to the image\n",
        "\n",
        "        Returns:\n",
        "            pandas.DataFrame: Percentage of each land class\n",
        "        \"\"\"\n",
        "        image = cv2.imread(image_path)\n",
        "        if image is None:\n",
        "            print(f\"Error: Image {image_path} not loaded properly.\")\n",
        "            return None\n",
        "\n",
        "        hsv = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
        "        total_pixels = image.shape[0] * image.shape[1]\n",
        "\n",
        "        class_percentages = {}\n",
        "        for class_name, (lower, upper) in self.color_ranges.items():\n",
        "            mask = cv2.inRange(hsv, lower, upper)\n",
        "            class_pixels = cv2.countNonZero(mask)\n",
        "            class_percentage = (class_pixels / total_pixels) * 100\n",
        "            class_percentages[class_name] = class_percentage\n",
        "\n",
        "        return pd.DataFrame.from_dict(class_percentages, orient='index', columns=['Percentage'])\n",
        "\n",
        "    def compare_images(self, image_path1, image_path2):\n",
        "        \"\"\"\n",
        "        Compare two satellite images and generate change summary\n",
        "\n",
        "        Args:\n",
        "            image_path1 (str): Path to first image\n",
        "            image_path2 (str): Path to second image\n",
        "\n",
        "        Returns:\n",
        "            dict: Detailed change analysis\n",
        "        \"\"\"\n",
        "        # Load color percentages\n",
        "        df1 = self.detect_class_color_portions(image_path1)\n",
        "        df2 = self.detect_class_color_portions(image_path2)\n",
        "\n",
        "        if df1 is None or df2 is None:\n",
        "            print(\"Error: Image analysis failed.\")\n",
        "            return None\n",
        "\n",
        "        # Compute differences\n",
        "        df_comparison = pd.merge(df1, df2, left_index=True, right_index=True,\n",
        "                                 suffixes=('_image1', '_image2'))\n",
        "        df_comparison['Difference'] = df_comparison['Percentage_image2'] - df_comparison['Percentage_image1']\n",
        "\n",
        "        # Generate change statements\n",
        "        change_statements = []\n",
        "        for class_name, row in df_comparison.iterrows():\n",
        "            percentage1 = row['Percentage_image1']\n",
        "            percentage2 = row['Percentage_image2']\n",
        "            difference = row['Difference']\n",
        "\n",
        "            if abs(difference) > 0.01:\n",
        "                change_type = \"increased\" if difference > 0 else \"reduced\"\n",
        "                statement = (\n",
        "                    f\"{class_name.replace('_', ' ').title()} has been {change_type} \"\n",
        "                    f\"from {percentage1:.2f}% to {percentage2:.2f}% \"\n",
        "                    f\"({abs(difference):.2f}% change).\"\n",
        "                )\n",
        "                change_statements.append(statement)\n",
        "\n",
        "        # Visualize images\n",
        "        img1 = cv2.imread(image_path1)\n",
        "        img2 = cv2.imread(image_path2)\n",
        "\n",
        "        if img1 is not None and img2 is not None:\n",
        "            height = min(img1.shape[0], img2.shape[0])\n",
        "            img1_resized = cv2.resize(img1, (int(img1.shape[1] * height / img1.shape[0]), height))\n",
        "            img2_resized = cv2.resize(img2, (int(img2.shape[1] * height / img2.shape[0]), height))\n",
        "            combined_image = np.hstack((img1_resized, img2_resized))\n",
        "            cv2_imshow(combined_image)\n",
        "\n",
        "        # Prepare areas difference table\n",
        "        areas_difference = df_comparison[['Difference', 'Percentage_image1', 'Percentage_image2']]\n",
        "        areas_difference.columns = ['Difference', 'Percentage Period 1', 'Percentage Period 2']\n",
        "\n",
        "        return {\n",
        "            'change_statements': change_statements,\n",
        "            'detailed_comparison': df_comparison,\n",
        "            'areas_difference': areas_difference\n",
        "        }\n",
        "\n",
        "    def generate_environmental_insights(self, change_analysis):\n",
        "        \"\"\"\n",
        "        Generate advanced environmental insights based on land use changes\n",
        "\n",
        "        Args:\n",
        "            change_analysis (dict): Change analysis from compare_images method\n",
        "\n",
        "        Returns:\n",
        "            dict: Comprehensive environmental insights\n",
        "        \"\"\"\n",
        "        if not change_analysis or not change_analysis['change_statements']:\n",
        "            return {\"summary\": \"No significant changes detected.\"}\n",
        "\n",
        "        insights = {\n",
        "            \"summary\": \"\",\n",
        "            \"detailed_insights\": {}\n",
        "        }\n",
        "\n",
        "        # Areas Difference Table\n",
        "        areas_difference = change_analysis['areas_difference']\n",
        "        insights['areas_difference'] = areas_difference.to_string()\n",
        "\n",
        "        for statement in change_analysis['change_statements']:\n",
        "            # Use regex to extract key information\n",
        "            match = re.match(r'(\\w+ \\w+) has been (\\w+) from (\\d+\\.\\d+)% to (\\d+\\.\\d+)% \\((\\d+\\.\\d+)% change\\).', statement)\n",
        "\n",
        "            if match:\n",
        "                class_name = match.group(1).lower().replace(' ', '_')\n",
        "                change_type = match.group(2)\n",
        "                percentage_change = float(match.group(5))\n",
        "\n",
        "                # Advanced AI-driven insights generation\n",
        "                if 'urban' in class_name:\n",
        "                    insights['detailed_insights'][class_name] = self._generate_urban_insights(\n",
        "                        change_type, percentage_change\n",
        "                    )\n",
        "\n",
        "                elif 'agriculture' in class_name:\n",
        "                    insights['detailed_insights'][class_name] = self._generate_agriculture_insights(\n",
        "                        change_type, percentage_change\n",
        "                    )\n",
        "\n",
        "                elif 'rangeland' in class_name:\n",
        "                    insights['detailed_insights'][class_name] = self._generate_rangeland_insights(\n",
        "                        change_type, percentage_change\n",
        "                    )\n",
        "\n",
        "                elif 'forest' in class_name:\n",
        "                    insights['detailed_insights'][class_name] = self._generate_forest_insights(\n",
        "                        change_type, percentage_change\n",
        "                    )\n",
        "\n",
        "                elif 'water' in class_name:\n",
        "                    insights['detailed_insights'][class_name] = self._generate_water_insights(\n",
        "                        change_type, percentage_change\n",
        "                    )\n",
        "\n",
        "        # Combine change statements into summary\n",
        "        insights['summary'] = \" \".join(change_analysis['change_statements'])\n",
        "\n",
        "        return insights\n",
        "\n",
        "    def _generate_urban_insights(self, change_type, percentage_change):\n",
        "        \"\"\"AI-driven urban land change insights\"\"\"\n",
        "        base_insights = [\n",
        "            \"Urban land transformations can signal profound socio-economic shifts.\",\n",
        "            \"Changes in urban landscapes reflect complex interactions between human development and environmental dynamics.\"\n",
        "        ]\n",
        "\n",
        "        if change_type == 'reduced':\n",
        "            adaptive_insights = [\n",
        "                f\"Significant urban land reduction ({percentage_change:.2f}%) suggests potential de-urbanization trends.\",\n",
        "                \"Possible factors include: economic decentralization, population migration, or environmental conservation efforts.\",\n",
        "                \"This reduction might indicate a shift towards more sustainable, distributed settlement patterns.\"\n",
        "            ]\n",
        "        else:\n",
        "            adaptive_insights = [\n",
        "                f\"Urban expansion of {percentage_change:.2f}% indicates accelerated urban development.\",\n",
        "                \"Potential implications include increased infrastructure demands, economic growth, and potential environmental pressures.\",\n",
        "                \"Recommend comprehensive urban planning to mitigate ecological footprint.\"\n",
        "            ]\n",
        "\n",
        "        return \"\\n\".join(base_insights + adaptive_insights)\n",
        "\n",
        "    def _generate_agriculture_insights(self, change_type, percentage_change):\n",
        "        \"\"\"AI-driven agriculture land change insights\"\"\"\n",
        "        base_insights = [\n",
        "            \"Agricultural land transformations are critical indicators of regional ecological and economic health.\",\n",
        "            \"Land use changes reflect complex interactions between farming practices, climate, and socio-economic factors.\"\n",
        "        ]\n",
        "\n",
        "        if change_type == 'increased':\n",
        "            adaptive_insights = [\n",
        "                f\"Substantial agricultural land increase ({percentage_change:.2f}%) suggests significant land-use reconfiguration.\",\n",
        "                \"Potential drivers: food security initiatives, climate adaptation strategies, or agricultural policy changes.\",\n",
        "                \"Recommend evaluating sustainability of expanded agricultural practices and potential ecosystem impacts.\"\n",
        "            ]\n",
        "        else:\n",
        "            adaptive_insights = [\n",
        "                f\"Agricultural land reduction of {percentage_change:.2f}% might indicate challenging agricultural conditions.\",\n",
        "                \"Possible factors: urbanization, land degradation, or shifts in agricultural productivity.\",\n",
        "                \"Urgent need for sustainable land management and diversification strategies.\"\n",
        "            ]\n",
        "\n",
        "        return \"\\n\".join(base_insights + adaptive_insights)\n",
        "\n",
        "    def _generate_rangeland_insights(self, change_type, percentage_change):\n",
        "        \"\"\"AI-driven rangeland change insights\"\"\"\n",
        "        base_insights = [\n",
        "            \"Rangeland transformations are sensitive indicators of ecological resilience and land-use dynamics.\",\n",
        "            \"Changes reflect complex interactions between livestock management, climate conditions, and ecosystem health.\"\n",
        "        ]\n",
        "\n",
        "        if change_type == 'reduced':\n",
        "            adaptive_insights = [\n",
        "                f\"Rangeland reduction of {percentage_change:.2f}% signals potential ecosystem stress.\",\n",
        "                \"Possible causes: overgrazing, climate change, land-use conversion, or habitat fragmentation.\",\n",
        "                \"Recommend comprehensive ecosystem assessment and adaptive management strategies.\"\n",
        "            ]\n",
        "        else:\n",
        "            adaptive_insights = [\n",
        "                f\"Slight rangeland increase of {percentage_change:.2f}% might indicate ecosystem recovery efforts.\",\n",
        "                \"Potential signals: improved land management, conservation initiatives, or climate adaptation.\",\n",
        "                \"Continued monitoring crucial for understanding long-term ecological trends.\"\n",
        "            ]\n",
        "\n",
        "        return \"\\n\".join(base_insights + adaptive_insights)\n",
        "\n",
        "    def _generate_forest_insights(self, change_type, percentage_change):\n",
        "        \"\"\"AI-driven forest land change insights\"\"\"\n",
        "        base_insights = [\n",
        "            \"Forest land transformations are critical indicators of global ecological health.\",\n",
        "            \"Changes reflect complex interactions between human activities, climate dynamics, and ecosystem resilience.\"\n",
        "        ]\n",
        "\n",
        "        if change_type == 'reduced':\n",
        "            adaptive_insights = [\n",
        "                f\"Dramatic forest land reduction of {percentage_change:.2f}% represents significant ecological concern.\",\n",
        "                \"Potential implications: biodiversity loss, carbon sequestration disruption, climate change acceleration.\",\n",
        "                \"Urgent need for comprehensive reforestation and conservation strategies.\"\n",
        "            ]\n",
        "        else:\n",
        "            adaptive_insights = [\n",
        "                f\"Forest land changes of {percentage_change:.2f}% might indicate restoration efforts.\",\n",
        "                \"Potential signals: successful conservation programs, changing land-use policies, or natural regeneration.\",\n",
        "                \"Recommend continued monitoring and supporting forest ecosystem recovery.\"\n",
        "            ]\n",
        "\n",
        "        return \"\\n\".join(base_insights + adaptive_insights)\n",
        "\n",
        "    def _generate_water_insights(self, change_type, percentage_change):\n",
        "        \"\"\"AI-driven water area change insights\"\"\"\n",
        "        base_insights = [\n",
        "            \"Water area transformations are crucial indicators of hydrological and ecological system dynamics.\",\n",
        "            \"Changes reflect complex interactions between climate, human activities, and environmental processes.\"\n",
        "        ]\n",
        "\n",
        "        if change_type == 'reduced':\n",
        "            adaptive_insights = [\n",
        "                f\"Water area reduction of {percentage_change:.2f}% suggests significant hydrological changes.\",\n",
        "                \"Possible factors: climate change, water resource management, agricultural expansion, or ecosystem shifts.\",\n",
        "                \"Urgent need for comprehensive watershed management and conservation strategies.\"\n",
        "            ]\n",
        "        else:\n",
        "            adaptive_insights = [\n",
        "                f\"Water area stabilization or increase of {percentage_change:.2f}% might indicate positive environmental trends.\",\n",
        "                \"Potential signals: improved water conservation, ecosystem restoration, or climate adaptation.\",\n",
        "                \"Continued monitoring essential for understanding water resource dynamics.\"\n",
        "            ]\n",
        "\n",
        "        return \"\\n\".join(base_insights + adaptive_insights)\n",
        "\n",
        "def main():\n",
        "    # Set paths\n",
        "    model_path = '/content/drive/MyDrive/Satellite Dataset/eighty.keras'\n",
        "    image_path1 = '/content/635741_sat.jpg'\n",
        "    image_path2 = '/content/89743_sat.jpg'\n",
        "\n",
        "    # Initialize analyzer\n",
        "    analyzer = SatelliteImageAnalyzer(model_path)\n",
        "\n",
        "    # Predict masks for both images\n",
        "    mask1 = analyzer.predict_mask(image_path1, 'mask1.png')\n",
        "    mask2 = analyzer.predict_mask(image_path2, 'mask2.png')\n",
        "\n",
        "    # Compare images and generate change analysis\n",
        "    change_analysis = analyzer.compare_images(\n",
        "        f'/content/drive/MyDrive/Satellite Dataset/Predictions/mask1.png',\n",
        "        f'/content/drive/MyDrive/Satellite Dataset/Predictions/mask2.png'\n",
        "    )\n",
        "\n",
        "    # Generate and print environmental insights\n",
        "    if change_analysis:\n",
        "        # Print Change Summary in multiple lines\n",
        "        print(\"Change Summary:\")\n",
        "        for statement in change_analysis['change_statements']:\n",
        "            print(statement)\n",
        "\n",
        "        # Print Areas Difference\n",
        "        print(\"\\n--- Areas Difference Over the Period ---\")\n",
        "        print(change_analysis['areas_difference'])\n",
        "\n",
        "        # Generate and print advanced environmental insights\n",
        "        insights = analyzer.generate_environmental_insights(change_analysis)\n",
        "\n",
        "        # Print Detailed Insights, including a check for water insights\n",
        "        print(\"\\n--- Environmental Insights ---\")\n",
        "        for category, insight in insights['detailed_insights'].items():\n",
        "            print(f\"\\n### {category.replace('_', ' ').title()} Land Transformation\")\n",
        "            print(insight)\n",
        "\n",
        "        # If no water insights were generated, specifically explain why\n",
        "        if not any('water' in category.lower() for category in insights['detailed_insights']):\n",
        "            print(\"\\n### Water Land Transformation\")\n",
        "            print(\"No significant water area changes detected in this analysis.\")\n",
        "\n",
        "# Run the main function\n",
        "if __name__ == '__main__':\n",
        "    main()"
      ],
      "metadata": {
        "id": "k9Yt7N1QI2px"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}