{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Production-Grade Satellite Segmentation (DeepGlobe)\n\nThis notebook rebuilds the original U-Net workflow into a production-grade, GPU-efficient training and inference pipeline for 1024\u00d71024 images, optimized for NVIDIA L4 (24 GB) and comparable GPUs.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Audience:\n- ML engineers or applied researchers deploying semantic segmentation.\n\nPrerequisites:\n- Familiarity with PyTorch, CUDA, and image segmentation.\n- Dataset available locally with `metadata.csv` and `class_dict.csv`.\n\nBy the end you will:\n- Train a 1024\u00d71024 model with mixed precision and robust metrics.\n- Run tiled inference and compute class-change analytics.\n- Save reproducible artifacts for production use.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Outline\n\n1. Setup and configuration\n2. Data loading and class encoding\n3. Datasets and augmentations\n4. Metrics\n5. Model options (U-Net and SegFormer)\n6. Training loop with AMP and checkpointing\n7. Inference, visualization, and tiling\n8. Change detection from predicted masks\n9. Exercises and extensions\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Step 0 - Setup and configuration\n\nThis cell configures the environment, GPU settings, and paths. Update `DATA_ROOT` to your dataset location.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "from __future__ import annotations\n\nimport os\nimport json\nimport math\nimport random\nfrom dataclasses import dataclass, asdict\nfrom pathlib import Path\nfrom typing import Dict, Tuple, List\n\nimport numpy as np\nimport pandas as pd\nimport cv2\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import models\nimport albumentations as A\nfrom tqdm import tqdm\n\n# Optional: transformers v5 for SegFormer\ntry:\n    from transformers import SegformerForSemanticSegmentation\n    TRANSFORMERS_AVAILABLE = True\nexcept Exception:\n    TRANSFORMERS_AVAILABLE = False\n\n# --------------------\n# Reproducibility\n# --------------------\nSEED = 42\nrandom.seed(SEED)\nnp.random.seed(SEED)\ntorch.manual_seed(SEED)\ntorch.cuda.manual_seed_all(SEED)\n\n# --------------------\n# CUDA / performance\n# --------------------\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nif DEVICE.type == \"cuda\":\n    torch.backends.cudnn.benchmark = True\n    torch.backends.cuda.matmul.allow_tf32 = True\n    torch.backends.cudnn.allow_tf32 = True\n    torch.set_float32_matmul_precision(\"high\")\n\nprint(\"Torch:\", torch.__version__)\nprint(\"CUDA:\", torch.version.cuda)\nif DEVICE.type == \"cuda\":\n    print(\"GPU:\", torch.cuda.get_device_name(0))\n\n@dataclass\nclass Config:\n    data_root: str = \".\"\n    metadata_csv: str = \"metadata.csv\"\n    class_dict_csv: str = \"class_dict.csv\"\n    run_dir: str = \"runs/l4_1024\"\n    image_size: int = 1024\n    batch_size: int = 2\n    num_workers: int = 4\n    lr: float = 2e-4\n    weight_decay: float = 1e-4\n    epochs: int = 40\n    grad_accum_steps: int = 2\n    amp: bool = True\n    model_type: str = \"unet_resnet34\"  # options: unet_resnet34, segformer_b2\n    save_every: int = 1\n\nCFG = Config()\n\nDATA_ROOT = Path(CFG.data_root)\nRUN_DIR = Path(CFG.run_dir)\nRUN_DIR.mkdir(parents=True, exist_ok=True)\n\nprint(f\"Device: {DEVICE}\")\nprint(f\"Run dir: {RUN_DIR.resolve()}\")\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Step 1 - Load metadata and class mapping\n\nWe use `metadata.csv` to build splits. If validation masks are missing, we create a deterministic validation split from the training set.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "metadata_path = DATA_ROOT / CFG.metadata_csv\nclass_dict_path = DATA_ROOT / CFG.class_dict_csv\n\nif not metadata_path.exists():\n    raise FileNotFoundError(f\"Missing metadata.csv at {metadata_path}\")\nif not class_dict_path.exists():\n    raise FileNotFoundError(f\"Missing class_dict.csv at {class_dict_path}\")\n\nmeta_df = pd.read_csv(metadata_path)\nclass_df = pd.read_csv(class_dict_path)\n\n# class_dict.csv uses row order as class index\nid2label = {i: name for i, name in enumerate(class_df[\"name\"].tolist())}\nlabel2id = {v: k for k, v in id2label.items()}\n\ncolors = class_df[[\"r\", \"g\", \"b\"]].values.tolist()\ncolor_to_id = {tuple(color): idx for idx, color in enumerate(colors)}\n\nNUM_CLASSES = len(colors)\nprint(f\"Classes: {NUM_CLASSES}\")\nprint(id2label)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Step 2 - Dataset, encoding, and augmentations\n\nMasks are encoded into class indices (0..C-1). We keep augmentation minimal but correct for segmentation (nearest-neighbor for masks).\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "def encode_mask(mask_rgb: np.ndarray, color_to_id: Dict[Tuple[int, int, int], int]) -> np.ndarray:\n    mask = np.zeros((mask_rgb.shape[0], mask_rgb.shape[1]), dtype=np.int64)\n    for color, idx in color_to_id.items():\n        mask[(mask_rgb == color).all(axis=-1)] = idx\n    return mask\n\ndef decode_mask(mask: np.ndarray, colors: List[List[int]]) -> np.ndarray:\n    out = np.zeros((mask.shape[0], mask.shape[1], 3), dtype=np.uint8)\n    for idx, color in enumerate(colors):\n        out[mask == idx] = color\n    return out\n\nclass DeepGlobeDataset(Dataset):\n    def __init__(self, df: pd.DataFrame, root: Path, transforms=None, with_masks: bool = True):\n        self.df = df.reset_index(drop=True)\n        self.root = Path(root)\n        self.transforms = transforms\n        self.with_masks = with_masks\n\n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        img_path = self.root / row[\"sat_image_path\"]\n        image = cv2.imread(str(img_path), cv2.IMREAD_COLOR)\n        if image is None:\n            raise FileNotFoundError(f\"Image not found: {img_path}\")\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n\n        mask = None\n        if self.with_masks and isinstance(row.get(\"mask_path\", None), str) and len(row[\"mask_path\"]):\n            mask_path = self.root / row[\"mask_path\"]\n            mask_rgb = cv2.imread(str(mask_path), cv2.IMREAD_COLOR)\n            if mask_rgb is None:\n                raise FileNotFoundError(f\"Mask not found: {mask_path}\")\n            mask_rgb = cv2.cvtColor(mask_rgb, cv2.COLOR_BGR2RGB)\n            mask = encode_mask(mask_rgb, color_to_id)\n\n        if self.transforms is not None:\n            if mask is not None:\n                augmented = self.transforms(image=image, mask=mask)\n                image, mask = augmented[\"image\"], augmented[\"mask\"]\n            else:\n                augmented = self.transforms(image=image)\n                image = augmented[\"image\"]\n\n        image = image.astype(np.float32) / 255.0\n        image = torch.from_numpy(image).permute(2, 0, 1)\n\n        if mask is not None:\n            mask = torch.from_numpy(mask.astype(np.int64))\n            return image, mask, row.get(\"image_id\", idx)\n\n        return image, row.get(\"image_id\", idx)\n\ntrain_tfms = A.Compose([\n    A.RandomCrop(CFG.image_size, CFG.image_size),\n    A.HorizontalFlip(p=0.5),\n    A.VerticalFlip(p=0.2),\n    A.RandomRotate90(p=0.5),\n    A.ColorJitter(p=0.2),\n])\n\nval_tfms = A.Compose([\n    A.CenterCrop(CFG.image_size, CFG.image_size)\n])\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Step 3 - Build splits and loaders\n\nIf validation masks are missing, we create a deterministic split from the training set. Test set is used for inference only.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "from sklearn.model_selection import train_test_split\n\nmeta_df[\"mask_path\"] = meta_df[\"mask_path\"].fillna(\"\")\n\ntrain_df = meta_df[meta_df[\"split\"] == \"train\"].copy()\nvalid_df = meta_df[meta_df[\"split\"] == \"valid\"].copy()\ntest_df = meta_df[meta_df[\"split\"] == \"test\"].copy()\n\nvalid_has_masks = (valid_df[\"mask_path\"].str.len() > 0).any()\n\nif not valid_has_masks:\n    train_df, valid_df = train_test_split(\n        train_df,\n        test_size=0.2,\n        random_state=SEED,\n        shuffle=True\n    )\n\ntrain_ds = DeepGlobeDataset(train_df, DATA_ROOT, transforms=train_tfms, with_masks=True)\nval_ds = DeepGlobeDataset(valid_df, DATA_ROOT, transforms=val_tfms, with_masks=True)\ntest_ds = DeepGlobeDataset(test_df, DATA_ROOT, transforms=val_tfms, with_masks=False)\n\ntrain_loader = DataLoader(\n    train_ds,\n    batch_size=CFG.batch_size,\n    shuffle=True,\n    num_workers=CFG.num_workers,\n    pin_memory=True,\n    persistent_workers=(CFG.num_workers > 0)\n)\n\nval_loader = DataLoader(\n    val_ds,\n    batch_size=CFG.batch_size,\n    shuffle=False,\n    num_workers=CFG.num_workers,\n    pin_memory=True,\n    persistent_workers=(CFG.num_workers > 0)\n)\n\nprint(f\"Train: {len(train_ds)} | Val: {len(val_ds)} | Test: {len(test_ds)}\")\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Step 4 - Metrics (mIoU + per-class IoU)\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "@torch.no_grad()\ndef compute_confusion(preds: torch.Tensor, targets: torch.Tensor, num_classes: int) -> torch.Tensor:\n    preds = preds.view(-1)\n    targets = targets.view(-1)\n    mask = (targets >= 0) & (targets < num_classes)\n    hist = torch.bincount(\n        num_classes * targets[mask] + preds[mask],\n        minlength=num_classes ** 2\n    ).reshape(num_classes, num_classes)\n    return hist\n\n@torch.no_grad()\ndef compute_iou(confusion: torch.Tensor) -> Tuple[float, Dict[int, float]]:\n    tp = torch.diag(confusion)\n    fp = confusion.sum(0) - tp\n    fn = confusion.sum(1) - tp\n    denom = tp + fp + fn + 1e-6\n    iou = (tp / denom).cpu().numpy()\n    miou = float(np.nanmean(iou))\n    per_class = {i: float(iou[i]) for i in range(len(iou))}\n    return miou, per_class\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Step 5 - Model options\n\nWe provide a production-ready U-Net (ResNet34 encoder) and an optional SegFormer model (transformers v5). Use `CFG.model_type` to select.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "class UNetResNet34(nn.Module):\n    def __init__(self, num_classes: int):\n        super().__init__()\n        encoder = models.resnet34(weights=models.ResNet34_Weights.DEFAULT)\n        self.layer0 = nn.Sequential(encoder.conv1, encoder.bn1, encoder.relu)\n        self.pool = encoder.maxpool\n        self.layer1 = encoder.layer1\n        self.layer2 = encoder.layer2\n        self.layer3 = encoder.layer3\n        self.layer4 = encoder.layer4\n\n        self.up4 = nn.ConvTranspose2d(512, 256, kernel_size=2, stride=2)\n        self.dec4 = nn.Sequential(nn.Conv2d(256 + 256, 256, 3, padding=1), nn.ReLU(inplace=True))\n\n        self.up3 = nn.ConvTranspose2d(256, 128, kernel_size=2, stride=2)\n        self.dec3 = nn.Sequential(nn.Conv2d(128 + 128, 128, 3, padding=1), nn.ReLU(inplace=True))\n\n        self.up2 = nn.ConvTranspose2d(128, 64, kernel_size=2, stride=2)\n        self.dec2 = nn.Sequential(nn.Conv2d(64 + 64, 64, 3, padding=1), nn.ReLU(inplace=True))\n\n        self.up1 = nn.ConvTranspose2d(64, 64, kernel_size=2, stride=2)\n        self.dec1 = nn.Sequential(nn.Conv2d(64 + 64, 64, 3, padding=1), nn.ReLU(inplace=True))\n\n        self.up0 = nn.ConvTranspose2d(64, 32, kernel_size=2, stride=2)\n        self.dec0 = nn.Sequential(nn.Conv2d(32, 32, 3, padding=1), nn.ReLU(inplace=True))\n\n        self.out = nn.Conv2d(32, num_classes, kernel_size=1)\n\n    def forward(self, x):\n        x0 = self.layer0(x)   # 1/2\n        x1 = self.layer1(self.pool(x0))  # 1/4\n        x2 = self.layer2(x1)  # 1/8\n        x3 = self.layer3(x2)  # 1/16\n        x4 = self.layer4(x3)  # 1/32\n\n        d4 = self.up4(x4)\n        d4 = self.dec4(torch.cat([d4, x3], dim=1))\n\n        d3 = self.up3(d4)\n        d3 = self.dec3(torch.cat([d3, x2], dim=1))\n\n        d2 = self.up2(d3)\n        d2 = self.dec2(torch.cat([d2, x1], dim=1))\n\n        d1 = self.up1(d2)\n        d1 = self.dec1(torch.cat([d1, x0], dim=1))\n\n        d0 = self.up0(d1)\n        d0 = self.dec0(d0)\n\n        return self.out(d0)\n\n\ndef build_model() -> nn.Module:\n    if CFG.model_type == \"unet_resnet34\":\n        return UNetResNet34(NUM_CLASSES)\n\n    if CFG.model_type == \"segformer_b2\":\n        if not TRANSFORMERS_AVAILABLE:\n            raise RuntimeError(\"transformers v5 is not available. Install transformers>=5 to use SegFormer.\")\n        model = SegformerForSemanticSegmentation.from_pretrained(\n            \"nvidia/segformer-b2-finetuned-ade-512-512\",\n            num_labels=NUM_CLASSES,\n            id2label=id2label,\n            label2id=label2id,\n            ignore_mismatched_sizes=True\n        )\n        return model\n\n    raise ValueError(f\"Unknown model type: {CFG.model_type}\")\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Step 6 - Training loop (AMP, gradient accumulation, checkpointing)\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "model = build_model().to(DEVICE)\n\ncriterion = nn.CrossEntropyLoss()\noptimizer = torch.optim.AdamW(model.parameters(), lr=CFG.lr, weight_decay=CFG.weight_decay)\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=CFG.epochs)\n\nscaler = torch.cuda.amp.GradScaler(enabled=(CFG.amp and DEVICE.type == \"cuda\"))\n\nbest_miou = -1.0\n\n@torch.no_grad()\ndef forward_logits(model, images):\n    out = model(images)\n    if isinstance(out, torch.Tensor):\n        return out\n    if hasattr(out, \"logits\"):\n        return out.logits\n    raise RuntimeError(\"Unexpected model output type\")\n\n\ndef train_one_epoch(epoch: int):\n    model.train()\n    running_loss = 0.0\n    for step, batch in enumerate(tqdm(train_loader, desc=f\"Train {epoch}\")):\n        images, masks, _ = batch\n        images = images.to(DEVICE, non_blocking=True)\n        masks = masks.to(DEVICE, non_blocking=True)\n\n        with torch.cuda.amp.autocast(enabled=(CFG.amp and DEVICE.type == \"cuda\")):\n            logits = forward_logits(model, images)\n            if logits.shape[-2:] != masks.shape[-2:]:\n                logits = F.interpolate(logits, size=masks.shape[-2:], mode=\"bilinear\", align_corners=False)\n            loss = criterion(logits, masks)\n            loss = loss / CFG.grad_accum_steps\n\n        scaler.scale(loss).backward()\n\n        if (step + 1) % CFG.grad_accum_steps == 0:\n            scaler.step(optimizer)\n            scaler.update()\n            optimizer.zero_grad(set_to_none=True)\n\n        running_loss += loss.item()\n\n    return running_loss / max(1, len(train_loader))\n\n@torch.no_grad()\ndef evaluate(epoch: int):\n    model.eval()\n    confusion = torch.zeros((NUM_CLASSES, NUM_CLASSES), device=DEVICE)\n    running_loss = 0.0\n\n    for batch in tqdm(val_loader, desc=f\"Val {epoch}\"):\n        images, masks, _ = batch\n        images = images.to(DEVICE, non_blocking=True)\n        masks = masks.to(DEVICE, non_blocking=True)\n\n        logits = forward_logits(model, images)\n        if logits.shape[-2:] != masks.shape[-2:]:\n            logits = F.interpolate(logits, size=masks.shape[-2:], mode=\"bilinear\", align_corners=False)\n\n        loss = criterion(logits, masks)\n        running_loss += loss.item()\n\n        preds = torch.argmax(logits, dim=1)\n        confusion += compute_confusion(preds, masks, NUM_CLASSES)\n\n    miou, per_class = compute_iou(confusion)\n    return running_loss / max(1, len(val_loader)), miou, per_class\n\nfor epoch in range(1, CFG.epochs + 1):\n    train_loss = train_one_epoch(epoch)\n    val_loss, val_miou, per_class = evaluate(epoch)\n    scheduler.step()\n\n    print(f\"Epoch {epoch:02d} | train_loss={train_loss:.4f} | val_loss={val_loss:.4f} | mIoU={val_miou:.4f}\")\n\n    if val_miou > best_miou:\n        best_miou = val_miou\n        ckpt_path = RUN_DIR / \"best_model.pt\"\n        torch.save({\n            \"model_state\": model.state_dict(),\n            \"config\": asdict(CFG),\n            \"miou\": val_miou\n        }, ckpt_path)\n        print(f\"Saved best model to {ckpt_path}\")\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Step 7 - Inference and visualization\n\nThis step loads the best checkpoint, runs a prediction, and saves a colorized mask.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "import matplotlib.pyplot as plt\n\nckpt = torch.load(RUN_DIR / \"best_model.pt\", map_location=DEVICE)\nmodel.load_state_dict(ckpt[\"model_state\"])\nmodel.eval()\n\nsample_image, sample_id = test_ds[0]\nimage = sample_image.unsqueeze(0).to(DEVICE)\n\nwith torch.no_grad():\n    logits = forward_logits(model, image)\n    preds = torch.argmax(logits, dim=1)[0].cpu().numpy()\n\npred_color = decode_mask(preds, colors)\n\nplt.figure(figsize=(10, 5))\nplt.subplot(1, 2, 1)\nplt.title(\"Input\")\nplt.imshow(sample_image.permute(1, 2, 0))\nplt.axis(\"off\")\n\nplt.subplot(1, 2, 2)\nplt.title(\"Predicted Mask\")\nplt.imshow(pred_color)\nplt.axis(\"off\")\nplt.show()\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Step 8 - Sliding window inference for large images\n\nUse this to run inference on 2048\u00d72048 images with a 1024\u00d71024 model.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "@torch.no_grad()\ndef sliding_window_predict(image: np.ndarray, tile_size: int = 1024, overlap: int = 128) -> np.ndarray:\n    h, w, _ = image.shape\n    stride = tile_size - overlap\n    full_probs = np.zeros((NUM_CLASSES, h, w), dtype=np.float32)\n    count = np.zeros((h, w), dtype=np.float32)\n\n    for y in range(0, h, stride):\n        for x in range(0, w, stride):\n            y1, x1 = y, x\n            y2, x2 = min(y1 + tile_size, h), min(x1 + tile_size, w)\n            tile = image[y1:y2, x1:x2]\n\n            pad_bottom = tile_size - (y2 - y1)\n            pad_right = tile_size - (x2 - x1)\n            if pad_bottom > 0 or pad_right > 0:\n                tile = cv2.copyMakeBorder(tile, 0, pad_bottom, 0, pad_right, cv2.BORDER_REFLECT_101)\n\n            tile_t = torch.from_numpy(tile.astype(np.float32) / 255.0).permute(2, 0, 1).unsqueeze(0).to(DEVICE)\n            logits = forward_logits(model, tile_t)\n            probs = torch.softmax(logits, dim=1)[0].cpu().numpy()\n\n            probs = probs[:, : (y2 - y1), : (x2 - x1)]\n            full_probs[:, y1:y2, x1:x2] += probs\n            count[y1:y2, x1:x2] += 1.0\n\n    full_probs /= np.maximum(count, 1e-6)\n    pred = np.argmax(full_probs, axis=0)\n    return pred\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Step 9 - Change detection using class-index masks\n\nThis avoids HSV heuristics and computes changes from class indices directly.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "def class_percentages(mask: np.ndarray, num_classes: int) -> Dict[int, float]:\n    total = mask.size\n    stats = {}\n    for c in range(num_classes):\n        stats[c] = float((mask == c).sum() / total * 100.0)\n    return stats\n\ndef compare_masks(mask_a: np.ndarray, mask_b: np.ndarray, id2label: Dict[int, str]):\n    p1 = class_percentages(mask_a, NUM_CLASSES)\n    p2 = class_percentages(mask_b, NUM_CLASSES)\n    rows = []\n    for k in range(NUM_CLASSES):\n        rows.append({\n            \"class\": id2label.get(k, str(k)),\n            \"period_1\": p1[k],\n            \"period_2\": p2[k],\n            \"difference\": p2[k] - p1[k]\n        })\n    return pd.DataFrame(rows)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Exercises\n\n1. Switch `CFG.model_type` to `segformer_b2` and compare mIoU.\n2. Increase `CFG.image_size` to 1536 and tune batch size + grad accumulation.\n3. Add class weights to `CrossEntropyLoss` based on training mask histograms.\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "execution_count": null,
      "outputs": [],
      "source": "# Exercise scaffold\n# TODO: Compute class weights from training masks and re-train the model.\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Pitfalls and extensions\n\nCommon pitfall:\n- Resizing masks with bilinear interpolation. Always use nearest neighbor for class labels.\n\nExtensions:\n- Add TensorBoard logging and model export (TorchScript or ONNX).\n- Add multi-GPU training with DDP if you move to larger instances.\n"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.10"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}